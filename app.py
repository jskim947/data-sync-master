"""
FS Master Web Application

ì›¹ ê¸°ë°˜ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
"""

from flask import Flask, render_template, request, jsonify, redirect, url_for, flash, abort
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from flask_cors import CORS
from datetime import datetime, timezone, timedelta
import os
import sys
import json
import configparser
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import threading
import time
import argparse
import psycopg2
from psycopg2 import OperationalError
import traceback

# db_connection_test import
from db_connection_test import get_server_config, connect_altibase, connect_informix, connect_postgresql, execute_query as db_execute_query

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'

# ì‹œê°„ëŒ€ ì„¤ì • (KST)
KST = timezone(timedelta(hours=9))
app.config['TIMEZONE'] = KST

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì½ê¸° (Docker í™˜ê²½)
database_url = os.environ.get('DATABASE_URL')
app.config['SQLALCHEMY_DATABASE_URI'] = database_url
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# CORS ì„¤ì •
CORS(app)

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
db = SQLAlchemy(app)
migrate = Migrate(app, db)

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (KST ì‹œê°„ëŒ€ ì‚¬ìš©)
scheduler = BackgroundScheduler(timezone='Asia/Seoul')
scheduler.start()

def get_kst_now():
    """í˜„ì¬ KST ì‹œê°„ ë°˜í™˜"""
    return datetime.now(KST)

def utc_to_kst(utc_time):
    """UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜"""
    if utc_time is None:
        return None
    if utc_time.tzinfo is None:
        utc_time = utc_time.replace(tzinfo=timezone.utc)
    return utc_time.astimezone(KST)

def kst_to_utc(kst_time):
    """KST ì‹œê°„ì„ UTCë¡œ ë³€í™˜"""
    if kst_time is None:
        return None
    if kst_time.tzinfo is None:
        kst_time = kst_time.replace(tzinfo=KST)
    return kst_time.astimezone(timezone.utc)

def format_kst_time(dt, format_str='%Y-%m-%d %H:%M:%S'):
    """KST ì‹œê°„ì„ í¬ë§·íŒ…"""
    if dt is None:
        return '-'
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    kst_time = dt.astimezone(KST)
    return kst_time.strftime(format_str)

def format_db_kst_time(dt, format_str='%Y-%m-%d %H:%M:%S'):
    """DBì— ì €ì¥ëœ KST ì‹œê°„ì„ í¬ë§·íŒ… (tzinfoê°€ Noneì¸ ê²½ìš° KSTë¡œ ê°€ì •)"""
    if dt is None:
        return '-'
    if dt.tzinfo is None:
        # DBì— ì €ì¥ëœ ì‹œê°„ì€ ì´ë¯¸ KSTì´ë¯€ë¡œ tzinfoë§Œ ì„¤ì •
        dt = dt.replace(tzinfo=KST)
    return dt.strftime(format_str)

def parse_cron_expression(cron_expr):
    """Cron í‘œí˜„ì‹ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    try:
        parts = cron_expr.split()
        if len(parts) != 5:
            return cron_expr
        
        minute, hour, day, month, weekday = parts
        
        # ìš”ì¼ ë§¤í•‘
        weekday_names = ['ì¼', 'ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ']
        
        # ë¶„ ì²˜ë¦¬
        if minute == '*':
            minute_desc = 'ëª¨ë“  ë¶„'
        elif ',' in minute:
            minute_desc = f"{minute.replace(',', ', ')}ë¶„"
        elif '-' in minute:
            start, end = minute.split('-')
            minute_desc = f"{start}~{end}ë¶„"
        else:
            minute_desc = f"{minute}ë¶„"
        
        # ì‹œê°„ ì²˜ë¦¬
        if hour == '*':
            hour_desc = 'ëª¨ë“  ì‹œê°„'
        elif ',' in hour:
            hours = hour.split(',')
            hour_desc = f"{', '.join(hours)}ì‹œ"
        elif '-' in hour:
            start, end = hour.split('-')
            hour_desc = f"{start}~{end}ì‹œ"
        else:
            hour_desc = f"{hour}ì‹œ"
        
        # ì¼ ì²˜ë¦¬
        if day == '*':
            day_desc = 'ëª¨ë“  ì¼'
        elif ',' in day:
            days = day.split(',')
            day_desc = f"{', '.join(days)}ì¼"
        elif '-' in day:
            start, end = day.split('-')
            day_desc = f"{start}~{end}ì¼"
        else:
            day_desc = f"{day}ì¼"
        
        # ì›” ì²˜ë¦¬
        if month == '*':
            month_desc = 'ëª¨ë“  ì›”'
        elif ',' in month:
            months = month.split(',')
            month_desc = f"{', '.join(months)}ì›”"
        elif '-' in month:
            start, end = month.split('-')
            month_desc = f"{start}~{end}ì›”"
        else:
            month_desc = f"{month}ì›”"
        
        # ìš”ì¼ ì²˜ë¦¬
        if weekday == '*':
            weekday_desc = 'ëª¨ë“  ìš”ì¼'
        elif ',' in weekday:
            weekdays = weekday.split(',')
            weekday_names_list = [weekday_names[int(w)] for w in weekdays]
            weekday_desc = f"{', '.join(weekday_names_list)}ìš”ì¼"
        elif '-' in weekday:
            start, end = weekday.split('-')
            start_name = weekday_names[int(start)]
            end_name = weekday_names[int(end)]
            weekday_desc = f"{start_name}~{end_name}ìš”ì¼"
        else:
            weekday_desc = f"{weekday_names[int(weekday)]}ìš”ì¼"
        
        # íŠ¹ë³„í•œ íŒ¨í„´ë“¤
        if cron_expr == '0 0 * * *':
            return 'ë§¤ì¼ ìì • (00:00)'
        elif cron_expr == '0 0 * * 1-5':
            return 'ì›”~ê¸ˆ ìì •'
        elif cron_expr == '0 0 * * 6-0':
            return 'í† ~ì¼ ìì •'
        elif cron_expr == '0 9 * * 1-5':
            return 'ì›”~ê¸ˆ ì˜¤ì „ 9ì‹œ'
        elif cron_expr == '0 12 * * 1-5':
            return 'ì›”~ê¸ˆ ì˜¤í›„ 12ì‹œ'
        elif cron_expr == '0 18 * * 1-5':
            return 'ì›”~ê¸ˆ ì˜¤í›„ 6ì‹œ'
        elif cron_expr == '20 5 * * 1-5':
            return 'ì›”~ê¸ˆ ì˜¤ì „ 5ì‹œ 20ë¶„'
        elif cron_expr == '20 12 * * 1-5':
            return 'ì›”~ê¸ˆ ì˜¤í›„ 12ì‹œ 20ë¶„'
        elif cron_expr == '0 9,18 * * 1-5':
            return 'ì›”~ê¸ˆ 9ì‹œ, 18ì‹œ'
        elif cron_expr == '0 */6 * * *':
            return '6ì‹œê°„ë§ˆë‹¤'
        elif cron_expr == '0 */12 * * *':
            return '12ì‹œê°„ë§ˆë‹¤'
        elif cron_expr == '0 */24 * * *':
            return '24ì‹œê°„ë§ˆë‹¤'
        
        # ì¼ë°˜ì ì¸ íŒ¨í„´
        if minute != '*' and hour != '*' and day == '*' and month == '*' and weekday == '*':
            return f"ë§¤ì¼ {hour}ì‹œ {minute}ë¶„"
        elif minute != '*' and hour != '*' and day == '*' and month == '*' and weekday != '*':
            return f"{weekday_desc} {hour}ì‹œ {minute}ë¶„"
        elif minute != '*' and hour != '*' and day != '*' and month == '*' and weekday == '*':
            return f"ë§¤ì›” {day}ì¼ {hour}ì‹œ {minute}ë¶„"
        
        # ë³µì¡í•œ íŒ¨í„´
        return f"{weekday_desc} {hour_desc} {minute_desc}"
        
    except Exception as e:
        return cron_expr

def test_informix_connection(conf, test_query):
    """Informix ì—°ê²° í…ŒìŠ¤íŠ¸ (ë³„ë„ í•¨ìˆ˜)"""
    import jaydebeapi
    import os
    
    # ì‹œë„í•  ë“œë¼ì´ë²„ í´ë˜ìŠ¤ëª…ë“¤ (JAR íŒŒì¼ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
    driver_classes = [
        "com.informix.jdbc.IfxDriver",  # ì‹¤ì œ JAR íŒŒì¼ì— ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤
        "com.informix.jdbc.driver.IfxDriver",
        "IfxDriver"
    ]
    
    # ì‹œë„í•  JAR íŒŒì¼ ê²½ë¡œë“¤ (ì ˆëŒ€ ê²½ë¡œ ìš°ì„ )
    jar_paths = [
        "/app/db_connection_test/ifxjdbc.jar",
        "/app/jdbc-drivers/ifxjdbc.jar",
        "ifxjdbc.jar",
        "./ifxjdbc.jar"
    ]
    
    # JAR íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    print("=== Informix JAR íŒŒì¼ í™•ì¸ ===")
    for jar in jar_paths:
        if os.path.exists(jar):
            print(f"âœ… JAR íŒŒì¼ ì¡´ì¬: {jar}")
        else:
            print(f"âŒ JAR íŒŒì¼ ì—†ìŒ: {jar}")
    
    for driver in driver_classes:
        for jar in jar_paths:
            try:
                # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
                original_cwd = os.getcwd()
                os.chdir('/app/db_connection_test')
                
                try:
                    print(f"\n=== Informix ì—°ê²° ì‹œë„ ===")
                    print(f"ë“œë¼ì´ë²„: {driver}")
                    print(f"JAR íŒŒì¼: {jar}")
                    print(f"í˜¸ìŠ¤íŠ¸: {conf['host']}:{conf['port']}")
                    print(f"ë°ì´í„°ë² ì´ìŠ¤: {conf['database']}")
                    print(f"ì‚¬ìš©ì: {conf['user']}")
                    
                    # ì§ì ‘ jaydebeapië¡œ ì—°ê²° (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
                    url = f"jdbc:informix-sqli://{conf['host']}:{conf['port']}/{conf['database']}:NEWCODESET=EUC-KR,cp1252,819"
                    print(f"ì—°ê²° URL: {url}")
                    
                    # JAR íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    if not os.path.exists(jar):
                        print(f"âŒ JAR íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {jar}")
                        continue
                    
                    print(f"JAR íŒŒì¼ í¬ê¸°: {os.path.getsize(jar)} bytes")
                    
                    # ì—°ê²° ì‹œë„
                    conn = jaydebeapi.connect(driver, url, [conf['user'], conf['password']], jar)
                    print(f"âœ… JDBC ì—°ê²° ì„±ê³µ")
                    
                    # Java ë ˆë²¨ì—ì„œ ì§ì ‘ ì ‘ê·¼
                    java_conn = conn.jconn
                    java_stmt = java_conn.createStatement()
                    java_stmt.setQueryTimeout(30)
                    
                    print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰: {test_query}")
                    java_result_set = java_stmt.executeQuery(test_query)
                    
                    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                    rows = []
                    while java_result_set.next():
                        meta_data = java_result_set.getMetaData()
                        column_count = meta_data.getColumnCount()
                        
                        row = []
                        for i in range(1, column_count + 1):
                            value = java_result_set.getObject(i)
                            row.append(value)
                        
                        rows.append(tuple(row))
                    
                    # ìì› ë°˜ë‚©
                    java_result_set.close()
                    java_stmt.close()
                    conn.close()
                    
                    print(f"âœ… Informix ì—°ê²° ì„±ê³µ: {driver}, {jar}")
                    print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {rows}")
                    return rows, True, None
                    
                except Exception as e:
                    print(f"âŒ Informix ì—°ê²° ì‹¤íŒ¨: {driver}, {jar}")
                    print(f"ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
                    import traceback
                    print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                    continue
                    
                finally:
                    # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                    os.chdir(original_cwd)
                    
            except Exception as e:
                print(f"âŒ Informix ì—°ê²° ì‹œë„ ì‹¤íŒ¨: {driver}, {jar} - {e}")
                continue
    
    error_msg = "ëª¨ë“  ë“œë¼ì´ë²„ í´ë˜ìŠ¤ì™€ JAR íŒŒì¼ ì‹œë„ ì‹¤íŒ¨"
    print(f"âŒ {error_msg}")
    return None, False, error_msg

def test_altibase_connection(conf, test_query):
    """Altibase ì—°ê²° í…ŒìŠ¤íŠ¸ (ë³„ë„ í•¨ìˆ˜)"""
    import jaydebeapi
    
    # ì‹œë„í•  ë“œë¼ì´ë²„ í´ë˜ìŠ¤ëª…ë“¤ (ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ)
    driver_classes = [
        "Altibase.jdbc.driver.AltibaseDriver",
        "com.altibase.jdbc.driver.AltibaseDriver"
    ]
    
    for driver in driver_classes:
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                # ì§ì ‘ jaydebeapië¡œ ì—°ê²° (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
                url = f"jdbc:Altibase://{conf['host']}:{conf['port']}/{conf['database']}"
                jar = "/app/db_connection_test/Altibase.jar"  # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
                
                conn = jaydebeapi.connect(driver, url, [conf['user'], conf['password']], jar)
                conn.autocommit = False
                
                # Java ë ˆë²¨ì—ì„œ ì§ì ‘ ì ‘ê·¼
                java_conn = conn.jconn
                java_stmt = java_conn.createStatement()
                java_stmt.setQueryTimeout(30)
                java_result_set = java_stmt.executeQuery(test_query)
                
                # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                rows = []
                while java_result_set.next():
                    meta_data = java_result_set.getMetaData()
                    column_count = meta_data.getColumnCount()
                    
                    row = []
                    for i in range(1, column_count + 1):
                        value = java_result_set.getObject(i)
                        row.append(value)
                    
                    rows.append(tuple(row))
                
                # ìì› ë°˜ë‚©
                java_result_set.close()
                java_stmt.close()
                conn.close()
                
                return rows, True, None
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            if driver == driver_classes[-1]:  # ë§ˆì§€ë§‰ ì‹œë„ì˜€ë‹¤ë©´
                return None, False, f"ëª¨ë“  ë“œë¼ì´ë²„ í´ë˜ìŠ¤ ì‹œë„ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {str(e)}"
            continue  # ë‹¤ìŒ ë“œë¼ì´ë²„ í´ë˜ìŠ¤ ì‹œë„
    
    return None, False, "ë“œë¼ì´ë²„ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def wait_for_postgres(max_retries=30, retry_interval=2):
    """PostgreSQLì´ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
    print("ğŸ—„ï¸ PostgreSQL ì—°ê²° ëŒ€ê¸° ì¤‘...")
    
    for attempt in range(max_retries):
        try:
            # PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸
            conn = psycopg2.connect(
                host=os.environ.get('POSTGRES_HOST', 'postgres'),
                port=os.environ.get('POSTGRES_PORT', 5432),
                database=os.environ.get('POSTGRES_DB', 'fs_master_web'),
                user=os.environ.get('POSTGRES_USER', 'postgres'),
                password=os.environ.get('POSTGRES_PASSWORD')
            )
            conn.close()
            print("âœ… PostgreSQL ì—°ê²° ì„±ê³µ!")
            return True
        except OperationalError as e:
            print(f"â³ PostgreSQL ì—°ê²° ì‹œë„ {attempt + 1}/{max_retries}: {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_interval)
            else:
                print("âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨")
                return False
    
    return False

def execute_query_with_columns(server_name, query):
    """ì»¬ëŸ¼ëª…ì„ í¬í•¨í•œ ì¿¼ë¦¬ ì‹¤í–‰"""
    import jaydebeapi
    import psycopg2
    import hashlib
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
    original_cwd = os.getcwd()
    os.chdir('/app/db_connection_test')
    
    try:
        conf = get_server_config(server_name)
        db_type = conf['type']
        
        if db_type == 'postgresql':
            # PostgreSQL ì—°ê²°
            conn = psycopg2.connect(
                host=conf['host'],
                port=conf['port'],
                database=conf['database'],
                user=conf['user'],
                password=conf['password']
            )
            
            cursor = conn.cursor()
            cursor.execute(query)
            
            # ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            columns = [desc[0] for desc in cursor.description]
            
            # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            rows = cursor.fetchall()
            
            # ê° í–‰ì— í•´ì‹œ ID ì¶”ê°€
            result_rows = []
            for i, row in enumerate(rows):
                # í–‰ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ìƒì„±
                row_str = str(row)
                row_hash = hashlib.md5(row_str.encode()).hexdigest()[:16]  # 16ìë¦¬ í•´ì‹œ
                
                # í•´ì‹œ IDë¥¼ ì²« ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
                new_row = (row_hash,) + row
                result_rows.append(new_row)
            
            # ì»¬ëŸ¼ëª…ì— ID ì»¬ëŸ¼ ì¶”ê°€
            result_columns = ['data_hash'] + columns
            
            cursor.close()
            conn.close()
            
            return result_columns, result_rows, True, None
            
        elif db_type == 'altibase':
            # Altibase ì—°ê²°
            url = f"jdbc:Altibase://{conf['host']}:{conf['port']}/{conf['database']}"
            jar = "/app/db_connection_test/Altibase.jar"
            
            conn = jaydebeapi.connect(
                "com.altibase.jdbc.driver.AltibaseDriver",
                url,
                [conf['user'], conf['password']],
                jar
            )
            
            java_conn = conn.jconn
            java_stmt = java_conn.createStatement()
            java_stmt.setQueryTimeout(30)
            java_result_set = java_stmt.executeQuery(query)
            
            # ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            meta_data = java_result_set.getMetaData()
            column_count = meta_data.getColumnCount()
            columns = []
            for i in range(1, column_count + 1):
                columns.append(meta_data.getColumnName(i))
            
            # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            rows = []
            while java_result_set.next():
                row = []
                for i in range(1, column_count + 1):
                    value = java_result_set.getObject(i)
                    row.append(value)
                rows.append(tuple(row))
            
            # ê° í–‰ì— í•´ì‹œ ID ì¶”ê°€
            result_rows = []
            for i, row in enumerate(rows):
                row_str = str(row)
                row_hash = hashlib.md5(row_str.encode()).hexdigest()[:16]
                new_row = (row_hash,) + row
                result_rows.append(new_row)
            
            # ì»¬ëŸ¼ëª…ì— ID ì»¬ëŸ¼ ì¶”ê°€
            result_columns = ['data_hash'] + columns
            
            java_result_set.close()
            java_stmt.close()
            conn.close()
            
            return result_columns, result_rows, True, None
            
        elif db_type == 'informix':
            # Informix ì—°ê²°
            url = f"jdbc:informix-sqli://{conf['host']}:{conf['port']}/{conf['database']}:NEWCODESET=EUC-KR,cp1252,819"
            jar = "/app/db_connection_test/ifxjdbc.jar"
            
            conn = jaydebeapi.connect(
                "com.informix.jdbc.IfxDriver",
                url,
                [conf['user'], conf['password']],
                jar
            )
            
            java_conn = conn.jconn
            java_stmt = java_conn.createStatement()
            java_stmt.setQueryTimeout(30)
            java_result_set = java_stmt.executeQuery(query)
            
            # ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
            meta_data = java_result_set.getMetaData()
            column_count = meta_data.getColumnCount()
            columns = []
            for i in range(1, column_count + 1):
                columns.append(meta_data.getColumnName(i))
            
            # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            rows = []
            while java_result_set.next():
                row = []
                for i in range(1, column_count + 1):
                    value = java_result_set.getObject(i)
                    row.append(value)
                rows.append(tuple(row))
            
            # ê° í–‰ì— í•´ì‹œ ID ì¶”ê°€
            result_rows = []
            for i, row in enumerate(rows):
                row_str = str(row)
                row_hash = hashlib.md5(row_str.encode()).hexdigest()[:16]
                new_row = (row_hash,) + row
                result_rows.append(new_row)
            
            # ì»¬ëŸ¼ëª…ì— ID ì»¬ëŸ¼ ì¶”ê°€
            result_columns = ['data_hash'] + columns
            
            java_result_set.close()
            java_stmt.close()
            conn.close()
            
            return result_columns, result_rows, True, None
            
        else:
            return None, None, False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {db_type}"
            
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return None, None, False, error_msg
    finally:
        # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
        os.chdir(original_cwd)

def execute_incremental_query(server_name, base_query, sync_key_column, last_sync_value, sync_strategy):
    """ì¦ë¶„ ë™ê¸°í™”ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ì‹¤í–‰"""
    import jaydebeapi
    import psycopg2
    import hashlib
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
    original_cwd = os.getcwd()
    os.chdir('/app/db_connection_test')
    
    try:
        conf = get_server_config(server_name)
        db_type = conf['type']
        
        # ì¦ë¶„ ë™ê¸°í™” ì¡°ê±´ ì¶”ê°€
        if sync_strategy == 'timestamp':
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™”
            if 'WHERE' in base_query.upper():
                incremental_query = f"{base_query} AND {sync_key_column} > '{last_sync_value}'"
            else:
                incremental_query = f"{base_query} WHERE {sync_key_column} > '{last_sync_value}'"
        elif sync_strategy == 'sequence':
            # ì‹œí€€ìŠ¤ ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™”
            if 'WHERE' in base_query.upper():
                incremental_query = f"{base_query} AND {sync_key_column} > {last_sync_value}"
            else:
                incremental_query = f"{base_query} WHERE {sync_key_column} > {last_sync_value}"
        elif sync_strategy == 'hash':
            # í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™” (ì „ì²´ í…Œì´ë¸” ìŠ¤ìº” í›„ í•´ì‹œ ë¹„êµ)
            # í•´ì‹œ ì „ëµì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„ íƒ€ê²Ÿê³¼ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ì œê±°
            incremental_query = base_query
        else:
            incremental_query = base_query
        
        print(f"ì¦ë¶„ ë™ê¸°í™” ì¿¼ë¦¬: {incremental_query}")
        
        # ê¸°ì¡´ execute_query_with_columns í•¨ìˆ˜ ì‚¬ìš©
        return execute_query_with_columns(server_name, incremental_query)
        
    except Exception as e:
        import traceback
        error_msg = f"ì¦ë¶„ ë™ê¸°í™” ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}"
        return None, None, False, error_msg
    finally:
        # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
        os.chdir(original_cwd)

def get_target_data_hashes(target_conf, table_name):
    """íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ í•´ì‹œê°’ë“¤ì„ ê°€ì ¸ì˜¤ê¸°"""
    try:
        target_db_type = target_conf['type']
        
        if target_db_type == 'postgresql':
            # PostgreSQLì—ì„œ í•´ì‹œê°’ ê°€ì ¸ì˜¤ê¸°
            import psycopg2
            conn = psycopg2.connect(
                host=target_conf['host'],
                port=target_conf['port'],
                database=target_conf['database'],
                user=target_conf['user'],
                password=target_conf['password']
            )
            
            cursor = conn.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' AND table_name = '{table_name}'
                );
            """)
            table_exists = cursor.fetchone()[0]
            
            if not table_exists:
                cursor.close()
                conn.close()
                return set()  # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ë¹ˆ ì§‘í•© ë°˜í™˜
            
            # data_hash ì»¬ëŸ¼ì—ì„œ í•´ì‹œê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
            cursor.execute(f"SELECT data_hash FROM {table_name}")
            existing_hashes = {row[0] for row in cursor.fetchall()}
            
            cursor.close()
            conn.close()
            return existing_hashes
            
        elif target_db_type == 'altibase':
            # Altibaseì—ì„œ í•´ì‹œê°’ ê°€ì ¸ì˜¤ê¸°
            import jaydebeapi
            
            url = f"jdbc:Altibase://{target_conf['host']}:{target_conf['port']}/{target_conf['database']}"
            jar = "/app/db_connection_test/Altibase.jar"
            
            conn = jaydebeapi.connect(
                "com.altibase.jdbc.driver.AltibaseDriver",
                url,
                [target_conf['user'], target_conf['password']],
                jar
            )
            
            cursor = conn.jconn.createStatement()
            
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                table_exists = True
            except:
                table_exists = False
            
            if not table_exists:
                conn.close()
                return set()
            
            # data_hash ì»¬ëŸ¼ì—ì„œ í•´ì‹œê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
            result_set = cursor.executeQuery(f"SELECT data_hash FROM {table_name}")
            
            existing_hashes = set()
            while result_set.next():
                existing_hashes.add(result_set.getString(1))
            
            result_set.close()
            conn.close()
            return existing_hashes
            
        elif target_db_type == 'informix':
            # Informixì—ì„œ í•´ì‹œê°’ ê°€ì ¸ì˜¤ê¸°
            import jaydebeapi
            
            url = f"jdbc:informix-sqli://{target_conf['host']}:{target_conf['port']}/{target_conf['database']}:NEWCODESET=EUC-KR,cp1252,819"
            jar = "/app/db_connection_test/ifxjdbc.jar"
            
            conn = jaydebeapi.connect(
                "com.informix.jdbc.IfxDriver",
                url,
                [target_conf['user'], target_conf['password']],
                jar
            )
            
            cursor = conn.jconn.createStatement()
            
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                table_exists = True
            except:
                table_exists = False
            
            if not table_exists:
                conn.close()
                return set()
            
            # data_hash ì»¬ëŸ¼ì—ì„œ í•´ì‹œê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
            result_set = cursor.executeQuery(f"SELECT data_hash FROM {table_name}")
            
            existing_hashes = set()
            while result_set.next():
                existing_hashes.add(result_set.getString(1))
            
            result_set.close()
            conn.close()
            return existing_hashes
            
        else:
            return set()
            
    except Exception as e:
        print(f"íƒ€ê²Ÿ ë°ì´í„° í•´ì‹œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return set()

def filter_new_data_by_hash(source_data, target_hashes):
    """í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë§Œ í•„í„°ë§"""
    if not source_data:
        return []
    
    new_data = []
    for row in source_data:
        if len(row) > 0:
            row_hash = row[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ data_hash
            if row_hash not in target_hashes:
                new_data.append(row)
    
    print(f"í•´ì‹œ ê¸°ë°˜ í•„í„°ë§: ì „ì²´ {len(source_data)}í–‰ ì¤‘ ìƒˆë¡œìš´ ë°ì´í„° {len(new_data)}í–‰")
    return new_data

def detect_changes(source_data, target_data, sync_key_column):
    """ë³€ê²½ì‚¬í•­ ê°ì§€ (ì¶”ê°€, ìˆ˜ì •, ì‚­ì œ)"""
    changes = {
        'added': [],
        'updated': [],
        'deleted': []
    }
    
    # í•´ì‹œ ê¸°ë°˜ ë³€ê²½ ê°ì§€
    source_dict = {}
    target_dict = {}
    
    # ì†ŒìŠ¤ ë°ì´í„°ë¥¼ í•´ì‹œë¡œ ë³€í™˜
    for row in source_data:
        if len(row) > 0:
            row_hash = row[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ í•´ì‹œ ID
            source_dict[row_hash] = row
    
    # íƒ€ê²Ÿ ë°ì´í„°ë¥¼ í•´ì‹œë¡œ ë³€í™˜
    for row in target_data:
        if len(row) > 0:
            row_hash = row[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ í•´ì‹œ ID
            target_dict[row_hash] = row
    
    # ì¶”ê°€ëœ í•­ëª©
    for row_hash in source_dict:
        if row_hash not in target_dict:
            changes['added'].append(source_dict[row_hash])
    
    # ì‚­ì œëœ í•­ëª©
    for row_hash in target_dict:
        if row_hash not in source_dict:
            changes['deleted'].append(target_dict[row_hash])
    
    # ìˆ˜ì •ëœ í•­ëª© (í•´ì‹œê°€ ê°™ì§€ë§Œ ë‚´ìš©ì´ ë‹¤ë¥¸ ê²½ìš°)
    for row_hash in source_dict:
        if row_hash in target_dict:
            if source_dict[row_hash] != target_dict[row_hash]:
                changes['updated'].append({
                    'old': target_dict[row_hash],
                    'new': source_dict[row_hash]
                })
    
    return changes

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class ServerConfig(db.Model):
    """ì„œë²„ ì„¤ì • ëª¨ë¸"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), unique=True, nullable=False)
    type = db.Column(db.String(50), nullable=False)
    host = db.Column(db.String(100), nullable=False)
    port = db.Column(db.Integer, nullable=False)
    database = db.Column(db.String(100), nullable=False)
    user = db.Column(db.String(100), nullable=False)
    password = db.Column(db.String(200), nullable=False)
    created_at = db.Column(db.DateTime, default=lambda: get_kst_now())
    updated_at = db.Column(db.DateTime, default=lambda: get_kst_now(), onupdate=lambda: get_kst_now())

class BatchJob(db.Model):
    """ë°°ì¹˜ ì‘ì—… ëª¨ë¸"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    description = db.Column(db.Text)
    source_server = db.Column(db.String(100), nullable=False)
    query = db.Column(db.Text, nullable=False)
    target_server = db.Column(db.String(100), nullable=False)
    target_table = db.Column(db.String(100), nullable=False)
    chunk_size = db.Column(db.Integer, default=10000)
    num_workers = db.Column(db.Integer, default=4)
    is_active = db.Column(db.Boolean, default=True)
    
    # ì¦ë¶„ ë™ê¸°í™” ê´€ë ¨ í•„ë“œ
    incremental_sync = db.Column(db.Boolean, default=False)  # ì¦ë¶„ ë™ê¸°í™” ì‚¬ìš© ì—¬ë¶€
    sync_key_column = db.Column(db.String(100))  # ë™ê¸°í™” í‚¤ ì»¬ëŸ¼ (ì˜ˆ: updated_at, id)
    last_sync_value = db.Column(db.String(200))  # ë§ˆì§€ë§‰ ë™ê¸°í™” ê°’
    sync_strategy = db.Column(db.String(50), default='timestamp')  # timestamp, sequence, hash
    
    created_at = db.Column(db.DateTime, default=lambda: get_kst_now())
    updated_at = db.Column(db.DateTime, default=lambda: get_kst_now(), onupdate=lambda: get_kst_now())

class BatchSchedule(db.Model):
    """ë°°ì¹˜ ìŠ¤ì¼€ì¤„ ëª¨ë¸"""
    id = db.Column(db.Integer, primary_key=True)
    job_id = db.Column(db.Integer, db.ForeignKey('batch_job.id'), nullable=False)
    cron_expression = db.Column(db.String(100), nullable=False)
    is_active = db.Column(db.Boolean, default=True)
    last_run = db.Column(db.DateTime)
    next_run = db.Column(db.DateTime)
    created_at = db.Column(db.DateTime, default=lambda: get_kst_now())
    
    job = db.relationship('BatchJob', backref=db.backref('schedules', lazy=True))

class BatchLog(db.Model):
    """ë°°ì¹˜ ì‹¤í–‰ ë¡œê·¸ ëª¨ë¸"""
    id = db.Column(db.Integer, primary_key=True)
    job_id = db.Column(db.Integer, db.ForeignKey('batch_job.id'), nullable=False)
    schedule_id = db.Column(db.Integer, db.ForeignKey('batch_schedule.id'))
    status = db.Column(db.String(20), nullable=False)  # success, failed, running
    total_rows = db.Column(db.Integer)
    total_size_mb = db.Column(db.Float)
    duration_seconds = db.Column(db.Float)
    rows_per_second = db.Column(db.Float)
    mb_per_second = db.Column(db.Float)
    error_message = db.Column(db.Text)
    started_at = db.Column(db.DateTime, default=lambda: get_kst_now())
    completed_at = db.Column(db.DateTime)
    
    job = db.relationship('BatchJob', backref=db.backref('logs', lazy=True))
    schedule = db.relationship('BatchSchedule', backref=db.backref('logs', lazy=True))

class QueryHistory(db.Model):
    """ì¿¼ë¦¬ ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ëª¨ë¸"""
    id = db.Column(db.Integer, primary_key=True)
    server_name = db.Column(db.String(100), nullable=False)
    query = db.Column(db.Text, nullable=False)
    result_count = db.Column(db.Integer)
    execution_time = db.Column(db.Float)
    status = db.Column(db.String(20), nullable=False)  # success, failed
    error_message = db.Column(db.Text)
    executed_at = db.Column(db.DateTime, default=lambda: get_kst_now())

# ì„¤ì • íŒŒì¼ ê´€ë¦¬
class ConfigManager:
    """ì„¤ì • íŒŒì¼ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config_file='/app/db_connection_test/db_servers.ini'):
        self.config_file = config_file
    
    def load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config = configparser.ConfigParser()
        if os.path.exists(self.config_file):
            config.read(self.config_file)
        return config
    
    def save_config(self, config):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        os.makedirs(os.path.dirname(self.config_file), exist_ok=True)
        with open(self.config_file, 'w', encoding='utf-8') as f:
            config.write(f)
    
    def sync_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„¤ì • íŒŒì¼ë¡œ ë™ê¸°í™”"""
        config = configparser.ConfigParser()
        
        servers = db.session.query(ServerConfig).all()
        for server in servers:
            config[server.name] = {
                'type': server.type,
                'host': server.host,
                'port': str(server.port),
                'database': server.database,
                'user': server.user,
                'password': server.password
            }
        
        self.save_config(config)
    
    def sync_to_database(self):
        """ì„¤ì • íŒŒì¼ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë™ê¸°í™”"""
        config = self.load_config()
        
        for section_name in config.sections():
            section = config[section_name]
            
            # ê¸°ì¡´ ì„œë²„ ì„¤ì • í™•ì¸
            existing = db.session.query(ServerConfig).filter_by(name=section_name).first()
            
            if existing:
                # ì—…ë°ì´íŠ¸
                existing.type = section['type']
                existing.host = section['host']
                existing.port = int(section['port'])
                existing.database = section['database']
                existing.user = section['user']
                existing.password = section['password']
            else:
                # ìƒˆë¡œ ìƒì„±
                server = ServerConfig(
                    name=section_name,
                    type=section['type'],
                    host=section['host'],
                    port=int(section['port']),
                    database=section['database'],
                    user=section['user'],
                    password=section['password']
                )
                db.session.add(server)
        
        db.session.commit()

# ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ê¸°
class BatchExecutor:
    """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
    
    def execute_job(self, job_id):
        """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰"""
        job = db.session.get(BatchJob, job_id)
        if not job:
            return False
        
        # ë¡œê·¸ ìƒì„±
        log = BatchLog(
            job_id=job.id,
            status='running',
            started_at=get_kst_now()
        )
        db.session.add(log)
        db.session.commit()
        
        try:
            # ì„¤ì • íŒŒì¼ ë™ê¸°í™”
            self.config_manager.sync_from_database()
            
            # ì¿¼ë¦¬ ì‹¤í–‰ (ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í•¨ìˆ˜ ì‚¬ìš©)
            start_time = time.time()
            
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                # ì„œë²„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                conf = get_server_config(job.source_server)
                db_type = conf['type']
                
                print(f"=== ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ ì‹œì‘ ===")
                print(f"ì‘ì—… ID: {job.id}")
                print(f"ì‘ì—…ëª…: {job.name}")
                print(f"ì†ŒìŠ¤ ì„œë²„: {job.source_server} ({db_type})")
                print(f"íƒ€ê²Ÿ ì„œë²„: {job.target_server}")
                print(f"ì¿¼ë¦¬: {job.query}")
                print(f"ì¦ë¶„ ë™ê¸°í™”: {job.incremental_sync}")
                if job.incremental_sync:
                    print(f"ë™ê¸°í™” ì „ëµ: {job.sync_strategy}")
                    print(f"ë™ê¸°í™” í‚¤ ì»¬ëŸ¼: {job.sync_key_column}")
                    print(f"ë§ˆì§€ë§‰ ë™ê¸°í™” ê°’: {job.last_sync_value}")
                
                # ì¦ë¶„ ë™ê¸°í™”ê°€ í™œì„±í™”ëœ ê²½ìš° ì¦ë¶„ ì¿¼ë¦¬ ì‹¤í–‰
                if job.incremental_sync and job.sync_key_column and job.sync_strategy:
                    print("ì¦ë¶„ ë™ê¸°í™” ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
                    columns, source_result, success, error = execute_incremental_query(
                        job.source_server, 
                        job.query, 
                        job.sync_key_column, 
                        job.last_sync_value, 
                        job.sync_strategy
                    )
                else:
                    # ì¼ë°˜ ì¿¼ë¦¬ ì‹¤í–‰ (ì „ì²´ ë™ê¸°í™”)
                    print("ì „ì²´ ë™ê¸°í™” ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
                    columns, source_result, success, error = execute_query_with_columns(job.source_server, job.query)
                
                if not success:
                    raise Exception(f"ì†ŒìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {error}")
                
                print(f"ì»¬ëŸ¼ëª…: {columns}")
                print(f"ë°ì´í„° í–‰ ìˆ˜: {len(source_result)}")
                print(f"ì²« ë²ˆì§¸ í–‰ ìƒ˜í”Œ: {source_result[0] if source_result else 'None'}")
                
                # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì €ì¥
                if source_result and len(source_result) > 0:
                    # íƒ€ê²Ÿ ì„œë²„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                    target_conf = get_server_config(job.target_server)
                    target_db_type = target_conf['type']
                    
                    print(f"íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {target_db_type}")
                    
                    # í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™”ì¸ ê²½ìš° ì¤‘ë³µ ë°ì´í„° í•„í„°ë§
                    if job.incremental_sync and job.sync_strategy == 'hash':
                        print("í•´ì‹œ ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™”: íƒ€ê²Ÿ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ì œê±° ì¤‘...")
                        target_hashes = get_target_data_hashes(target_conf, job.target_table)
                        print(f"íƒ€ê²Ÿ í…Œì´ë¸” ê¸°ì¡´ í•´ì‹œ ìˆ˜: {len(target_hashes)}")
                        
                        # ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ë°ì´í„°ë§Œ í•„í„°ë§
                        filtered_result = filter_new_data_by_hash(source_result, target_hashes)
                        
                        if filtered_result:
                            print(f"ì¤‘ë³µ ì œê±° í›„ ìƒˆë¡œìš´ ë°ì´í„°: {len(filtered_result)}í–‰")
                            source_result = filtered_result
                        else:
                            print("ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                            # ë¡œê·¸ ì—…ë°ì´íŠ¸
                            log.status = 'success'
                            log.total_rows = 0
                            log.total_size_mb = 0
                            log.duration_seconds = time.time() - start_time
                            log.rows_per_second = 0
                            log.mb_per_second = 0
                            log.completed_at = get_kst_now()
                            db.session.commit()
                            print("ë°°ì¹˜ ì‘ì—… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ (ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ)")
                            return True
                    
                    # ë™ê¸°í™” ì „ëµì— ë”°ë¥¸ ë°ì´í„° ì €ì¥ ë°©ì‹ ê²°ì •
                    sync_mode = "incremental" if job.incremental_sync else "full"
                    
                    # íƒ€ê²Ÿ í…Œì´ë¸”ì— ë°ì´í„° ì‚½ì… (ì»¬ëŸ¼ëª… í¬í•¨)
                    if target_db_type == 'postgresql':
                        # PostgreSQLì— ì €ì¥
                        target_success, target_error, target_result = self._save_to_postgresql_with_columns(
                            target_conf, job.target_table, source_result, columns, sync_mode
                        )
                    elif target_db_type == 'altibase':
                        # Altibaseì— ì €ì¥
                        target_success, target_error, target_result = self._save_to_altibase_with_columns(
                            target_conf, job.target_table, source_result, columns, sync_mode
                        )
                    elif target_db_type == 'informix':
                        # Informixì— ì €ì¥
                        target_success, target_error, target_result = self._save_to_informix_with_columns(
                            target_conf, job.target_table, source_result, columns, sync_mode
                        )
                    else:
                        raise Exception(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {target_db_type}")
                    
                    if not target_success:
                        raise Exception(f"íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {target_error}")
                    else:
                        print(f"íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì„±ê³µ: {target_db_type}")
                        
                        # ì¦ë¶„ ë™ê¸°í™”ê°€ í™œì„±í™”ëœ ê²½ìš° ë§ˆì§€ë§‰ ë™ê¸°í™” ê°’ ì—…ë°ì´íŠ¸
                        if job.incremental_sync and source_result:
                            # ë§ˆì§€ë§‰ í–‰ì˜ ë™ê¸°í™” í‚¤ ê°’ì„ ì—…ë°ì´íŠ¸
                            last_row = source_result[-1]
                            if job.sync_strategy == 'hash':
                                # í•´ì‹œ ì „ëµì¸ ê²½ìš° data_hash ì‚¬ìš©
                                job.last_sync_value = last_row[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ data_hash
                                print(f"ë§ˆì§€ë§‰ ë™ê¸°í™” ê°’ ì—…ë°ì´íŠ¸ (í•´ì‹œ): {job.last_sync_value}")
                            elif job.sync_key_column in last_row:
                                job.last_sync_value = str(last_row[job.sync_key_column])
                                print(f"ë§ˆì§€ë§‰ ë™ê¸°í™” ê°’ ì—…ë°ì´íŠ¸: {job.last_sync_value}")
                else:
                    print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì „ì²´ ë™ê¸°í™” ëª¨ë“œì—ì„œëŠ” ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                    if not job.incremental_sync:
                        print("ì „ì²´ ë™ê¸°í™” ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ìˆ˜í–‰")
                        target_conf = get_server_config(job.target_server)
                        target_db_type = target_conf['type']
                        
                        if target_db_type == 'postgresql':
                            self._clear_postgresql_table(target_conf, job.target_table)
                        elif target_db_type == 'altibase':
                            self._clear_altibase_table(target_conf, job.target_table)
                        elif target_db_type == 'informix':
                            self._clear_informix_table(target_conf, job.target_table)
                
                end_time = time.time()
                print(f"ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: {end_time - start_time:.2f}ì´ˆ")
                
                # ë¡œê·¸ ì—…ë°ì´íŠ¸
                log.status = 'success'
                log.total_rows = len(source_result) if source_result else 0
                log.total_size_mb = len(source_result) * 0.001 if source_result else 0  # ëŒ€ëµì ì¸ í¬ê¸°
                log.duration_seconds = end_time - start_time
                log.rows_per_second = len(source_result) / (end_time - start_time) if source_result and (end_time - start_time) > 0 else 0
                log.mb_per_second = log.total_size_mb / (end_time - start_time) if (end_time - start_time) > 0 else 0
                log.completed_at = get_kst_now()
                
                print("ë¡œê·¸ ì—…ë°ì´íŠ¸ ì¤‘...")
                db.session.commit()
                print("ë°°ì¹˜ ì‘ì—… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
                return True
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
            
        except Exception as e:
            # ì—ëŸ¬ ë¡œê·¸
            log.status = 'failed'
            log.error_message = str(e)
            log.completed_at = get_kst_now()
            db.session.commit()
            return False

    def _save_to_postgresql(self, conf, table_name, data):
        """PostgreSQLì— ë°ì´í„° ì €ì¥"""
        import psycopg2
        from psycopg2 import OperationalError
        
        try:
            print(f"PostgreSQL ì €ì¥ ì‹œì‘: {conf['host']}:{conf['port']}/{conf['database']}")
            print(f"í…Œì´ë¸”ëª…: {table_name}, ë°ì´í„° í–‰ ìˆ˜: {len(data)}")
            
            conn = psycopg2.connect(
                host=conf['host'],
                port=conf['port'],
                database=conf['database'],
                user=conf['user'],
                password=conf['password']
            )
            conn.autocommit = False  # íŠ¸ëœì­ì…˜ ìˆ˜ë™ ê´€ë¦¬
            
            cursor = conn.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
            cursor.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' AND table_name = '{table_name}'
                );
            """)
            table_exists = cursor.fetchone()[0]
            print(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
            
            if not table_exists:
                # ì›ë³¸ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±
                if data and len(data) > 0:
                    # ì²« ë²ˆì§¸ í–‰ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„
                    first_row = data[0]
                    print(f"ì²« ë²ˆì§¸ í–‰ íƒ€ì…: {type(first_row)}, ê¸¸ì´: {len(first_row) if isinstance(first_row, (list, tuple)) else 'N/A'}")
                    
                    if isinstance(first_row, (list, tuple)):
                        # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                        columns = []
                        for i, value in enumerate(first_row):
                            if isinstance(value, int):
                                columns.append(f"col_{i} INTEGER")
                            elif isinstance(value, float):
                                columns.append(f"col_{i} NUMERIC")
                            elif isinstance(value, bool):
                                columns.append(f"col_{i} BOOLEAN")
                            else:
                                columns.append(f"col_{i} TEXT")
                        
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id SERIAL PRIMARY KEY,
                                {', '.join(columns)}
                            );
                        """
                        print(f"ìƒì„±í•  í…Œì´ë¸” SQL: {create_sql}")
                    else:
                        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id SERIAL PRIMARY KEY,
                                data JSONB NOT NULL
                            );
                        """
                else:
                    # ê¸°ë³¸ êµ¬ì¡°
                    create_sql = f"""
                        CREATE TABLE {table_name} (
                            id SERIAL PRIMARY KEY,
                            data JSONB NOT NULL
                        );
                    """
                
                cursor.execute(create_sql)
                conn.commit()
                print(f"í…Œì´ë¸” {table_name}ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
            batch_size = 1000
            total_rows = len(data)
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {total_rows}í–‰, ë°°ì¹˜ í¬ê¸°: {batch_size}")
            
            # ë°ì´í„° ì‚½ì… (ë°°ì¹˜ ì²˜ë¦¬)
            for i in range(0, total_rows, batch_size):
                batch_data = data[i:i + batch_size]
                print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘: {i+1}~{min(i + batch_size, total_rows)}í–‰")
                
                if isinstance(batch_data[0], (list, tuple)):
                    # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                    values = []
                    for row in batch_data:
                        placeholders = ', '.join(['%s'] * len(row))
                        values.append(cursor.mogrify(f"({placeholders})", row).decode('utf-8'))
                    
                    if values:
                        # ì»¬ëŸ¼ëª…ì„ ë™ì ìœ¼ë¡œ ìƒì„±
                        column_names = [f"col_{i}" for i in range(len(batch_data[0]))]
                        column_list = ', '.join(column_names)
                        
                        insert_sql = f"""
                            INSERT INTO {table_name} ({column_list}) 
                            VALUES {','.join(values)};
                        """
                        print(f"INSERT SQL ì‹¤í–‰: {len(values)}í–‰")
                        cursor.execute(insert_sql)
                else:
                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                    values = []
                    for row in batch_data:
                        values.append(cursor.mogrify("(%s)", (json.dumps(row, ensure_ascii=False),)).decode('utf-8'))
                    
                    if values:
                        cursor.execute(f"""
                            INSERT INTO {table_name} (data) VALUES {','.join(values)};
                        """)
                
                # ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹
                conn.commit()
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, total_rows)}/{total_rows}")
            
            print("PostgreSQL ì €ì¥ ì™„ë£Œ")
            return True, None, None
            
        except OperationalError as e:
            error_msg = f"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}"
            print(error_msg)
            return False, error_msg, None
        except Exception as e:
            import traceback
            error_msg = f"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\n{traceback.format_exc()}"
            print(error_msg)
            return False, error_msg, None
        finally:
            if 'conn' in locals() and conn:
                conn.close()

    def _save_to_altibase(self, conf, table_name, data):
        """Altibaseì— ë°ì´í„° ì €ì¥"""
        import jaydebeapi
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                url = f"jdbc:Altibase://{conf['host']}:{conf['port']}/{conf['database']}"
                jar = "/app/db_connection_test/Altibase.jar"
                
                conn = jaydebeapi.connect(
                    "com.altibase.jdbc.driver.AltibaseDriver",
                    url,
                    [conf['user'], conf['password']],
                    jar
                )
                conn.autocommit = False  # íŠ¸ëœì­ì…˜ ìˆ˜ë™ ê´€ë¦¬
                
                cursor = conn.jconn.createStatement()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
                try:
                    cursor.execute(f"""
                        SELECT COUNT(*) FROM {table_name}
                    """)
                    table_exists = True
                except:
                    table_exists = False
                
                if not table_exists:
                    # ì›ë³¸ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±
                    if data and len(data) > 0:
                        first_row = data[0]
                        if isinstance(first_row, (list, tuple)) and len(first_row) <= 10:  # AltibaseëŠ” ì»¬ëŸ¼ ìˆ˜ ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŒ
                            # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (ì»¬ëŸ¼ ìˆ˜ê°€ ì ì„ ë•Œë§Œ)
                            columns = []
                            for i, value in enumerate(first_row):
                                if isinstance(value, int):
                                    columns.append(f"col_{i} INTEGER")
                                elif isinstance(value, float):
                                    columns.append(f"col_{i} NUMERIC")
                                elif isinstance(value, bool):
                                    columns.append(f"col_{i} SMALLINT")  # Altibaseì—ì„œ BOOLEAN ëŒ€ì‹  SMALLINT ì‚¬ìš©
                                else:
                                    columns.append(f"col_{i} VARCHAR(1000)")
                            
                            create_sql = f"""
                                CREATE TABLE {table_name} (
                                    id INTEGER PRIMARY KEY,
                                    {', '.join(columns)}
                                );
                            """
                        else:
                            # ì»¬ëŸ¼ì´ ë§ê±°ë‚˜ ë³µì¡í•œ ê²½ìš° JSON í˜•íƒœë¡œ ì €ì¥
                            create_sql = f"""
                                CREATE TABLE {table_name} (
                                    id INTEGER PRIMARY KEY,
                                    data VARCHAR(4000)
                                );
                            """
                    else:
                        # ê¸°ë³¸ êµ¬ì¡°
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id INTEGER PRIMARY KEY,
                                data VARCHAR(4000)
                            );
                        """
                    
                    cursor.execute(create_sql)
                    conn.jconn.commit()
                    print(f"í…Œì´ë¸” {table_name}ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
                batch_size = 1000
                total_rows = len(data)
                
                # ë°ì´í„° ì‚½ì… (ë°°ì¹˜ ì²˜ë¦¬)
                for i in range(0, total_rows, batch_size):
                    batch_data = data[i:i + batch_size]
                    
                    for j, row in enumerate(batch_data):
                        row_id = i + j + 1  # ê³ ìœ í•œ ID ìƒì„±
                        
                        # í…Œì´ë¸” êµ¬ì¡°ì— ë”°ë¼ ì ì ˆí•œ INSERT ì‚¬ìš©
                        if isinstance(row, (list, tuple)) and len(row) <= 10:
                            # ì»¬ëŸ¼ë³„ ì €ì¥
                            placeholders = ', '.join(['?'] * len(row))
                            column_names = [f"col_{k}" for k in range(len(row))]
                            column_list = ', '.join(column_names)
                            
                            cursor.execute(f"""
                                INSERT INTO {table_name} (id, {column_list}) VALUES (?, {placeholders});
                            """, [row_id] + list(row))
                        else:
                            # JSON í˜•íƒœë¡œ ì €ì¥
                            row_data = json.dumps(row, ensure_ascii=False)[:3990]  # Altibase VARCHAR ì œí•œ
                            
                            cursor.execute(f"""
                                INSERT INTO {table_name} (id, data) VALUES (?, ?);
                            """, (row_id, row_data))
                    
                    # ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹
                    conn.jconn.commit()
                    print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, total_rows)}/{total_rows}")
                
                return True, None, None
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            return False, f"Altibase ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", None

    def _save_to_informix(self, conf, table_name, data):
        """Informixì— ë°ì´í„° ì €ì¥"""
        import jaydebeapi
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                url = f"jdbc:informix-sqli://{conf['host']}:{conf['port']}/{conf['database']}:NEWCODESET=EUC-KR,cp1252,819"
                jar = "/app/db_connection_test/ifxjdbc.jar"
                
                conn = jaydebeapi.connect(
                    "com.informix.jdbc.IfxDriver",
                    url,
                    [conf['user'], conf['password']],
                    jar
                )
                conn.autocommit = True # ìë™ ì»¤ë°‹ í™œì„±í™”
                
                cursor = conn.jconn.createStatement()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
                cursor.execute(f"""
                    SELECT EXISTS (
                        SELECT FROM DUAL
                        WHERE EXISTS (
                            SELECT FROM sysmaster:sys_tables WHERE table_name = '{table_name}'
                        )
                    );
                """)
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    cursor.execute(f"""
                        CREATE TABLE {table_name} (
                            id INTEGER PRIMARY KEY,
                            data JSONB NOT NULL
                        );
                    """)
                    conn.jconn.commit()
                    print(f"í…Œì´ë¸” {table_name}ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ë°ì´í„° ì‚½ì…
                for row in data:
                    cursor.execute(f"""
                        INSERT INTO {table_name} (id, data) VALUES (?, ?);
                    """, (len(data) + 1, json.dumps(row))) # InformixëŠ” ìë™ ì¦ê°€ í‚¤ ì‚¬ìš©
                
                conn.jconn.commit()
                return True, None, None
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            return False, f"Informix ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", None

    def _save_to_postgresql_with_columns(self, conf, table_name, data, columns, sync_mode="full"):
        """PostgreSQLì— ë°ì´í„° ì €ì¥ (ì»¬ëŸ¼ëª… í¬í•¨) - ë™ê¸°í™” ì „ëµ ì ìš©"""
        import psycopg2
        from psycopg2 import OperationalError
        
        try:
            print(f"PostgreSQL ì €ì¥ ì‹œì‘: {conf['host']}:{conf['port']}/{conf['database']}")
            print(f"í…Œì´ë¸”ëª…: {table_name}, ë°ì´í„° í–‰ ìˆ˜: {len(data)}, ë™ê¸°í™” ëª¨ë“œ: {sync_mode}")
            
            conn = psycopg2.connect(
                host=conf['host'],
                port=conf['port'],
                database=conf['database'],
                user=conf['user'],
                password=conf['password']
            )
            conn.autocommit = False  # íŠ¸ëœì­ì…˜ ìˆ˜ë™ ê´€ë¦¬
            
            cursor = conn.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
            cursor.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' AND table_name = '{table_name}'
                );
            """)
            table_exists = cursor.fetchone()[0]
            print(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
            
            if not table_exists:
                # ì›ë³¸ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±
                if data and len(data) > 0:
                    # ì²« ë²ˆì§¸ í–‰ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„
                    first_row = data[0]
                    print(f"ì²« ë²ˆì§¸ í–‰ íƒ€ì…: {type(first_row)}, ê¸¸ì´: {len(first_row) if isinstance(first_row, (list, tuple)) else 'N/A'}")
                    
                    if isinstance(first_row, (list, tuple)):
                        # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id SERIAL PRIMARY KEY,
                                {', '.join([f'{col} TEXT' for col in columns])}
                            );
                        """
                        print(f"ìƒì„±í•  í…Œì´ë¸” SQL: {create_sql}")
                    else:
                        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id SERIAL PRIMARY KEY,
                                data JSONB NOT NULL
                            );
                        """
                else:
                    # ê¸°ë³¸ êµ¬ì¡°
                    create_sql = f"""
                        CREATE TABLE {table_name} (
                            id SERIAL PRIMARY KEY,
                            data JSONB NOT NULL
                        );
                    """
                
                cursor.execute(create_sql)
                conn.commit()
                print(f"í…Œì´ë¸” {table_name}ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ë™ê¸°í™” ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            if sync_mode == "full":
                # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…
                print("ì „ì²´ ë™ê¸°í™” ì „ëµ ì ìš©: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…")
                
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                cursor.execute(f"DELETE FROM {table_name}")
                deleted_count = cursor.rowcount
                print(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}í–‰")
                
                # ìƒˆ ë°ì´í„° ì‚½ì…
                if data and len(data) > 0:
                    self._insert_postgresql_data(cursor, table_name, data, columns)
                    print(f"PostgreSQL ì „ì²´ ë™ê¸°í™” ì™„ë£Œ: ì‚­ì œ {deleted_count}í–‰, ì‚½ì… {len(data)}í–‰")
                else:
                    print(f"PostgreSQL ì „ì²´ ë™ê¸°í™” ì™„ë£Œ: ì‚­ì œ {deleted_count}í–‰, ì‚½ì… 0í–‰")
                    
            elif sync_mode == "incremental":
                # ì¦ë¶„ ë™ê¸°í™”: ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€ (ì‚­ì œëœ ë°ì´í„°ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”)
                print("ì¦ë¶„ ë™ê¸°í™” ì „ëµ ì ìš©: ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€")
                
                if data and len(data) > 0:
                    # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„ì‹œ í…Œì´ë¸” ì‚¬ìš©
                    temp_table = f"{table_name}_temp_{int(time.time())}"
                    
                    # ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì‚½ì…
                    cursor.execute(f"CREATE TABLE {temp_table} AS SELECT * FROM {table_name} WHERE 1=0")
                    self._insert_postgresql_data(cursor, temp_table, data, columns)
                    
                    # ê¸°ì¡´ í…Œì´ë¸”ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    if isinstance(data[0], (list, tuple)) and len(columns) > 0:
                        # ì»¬ëŸ¼ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                        merge_sql = f"""
                            INSERT INTO {table_name} ({', '.join(columns)})
                            SELECT {', '.join(columns)} FROM {temp_table}
                            ON CONFLICT DO NOTHING;
                        """
                    else:
                        # JSON ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                        merge_sql = f"""
                            INSERT INTO {table_name} (data)
                            SELECT data FROM {temp_table}
                            ON CONFLICT DO NOTHING;
                        """
                    
                    cursor.execute(merge_sql)
                    inserted_count = cursor.rowcount
                    
                    # ì„ì‹œ í…Œì´ë¸” ì‚­ì œ
                    cursor.execute(f"DROP TABLE {temp_table}")
                    
                    print(f"PostgreSQL ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ: ì¶”ê°€ {inserted_count}í–‰")
                else:
                    print("PostgreSQL ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ: ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
            
            conn.commit()
            return True, None, None
            
        except OperationalError as e:
            error_msg = f"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}"
            print(error_msg)
            return False, error_msg, None
        except Exception as e:
            import traceback
            error_msg = f"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\n{traceback.format_exc()}"
            print(error_msg)
            return False, error_msg, None
        finally:
            if 'conn' in locals() and conn:
                conn.close()
    
    def _insert_postgresql_data(self, cursor, table_name, data, columns):
        """PostgreSQLì— ë°ì´í„° ì‚½ì… (í—¬í¼ í•¨ìˆ˜)"""
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        batch_size = 1000
        total_rows = len(data)
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {total_rows}í–‰, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°ì´í„° ì‚½ì… (ë°°ì¹˜ ì²˜ë¦¬)
        for i in range(0, total_rows, batch_size):
            batch_data = data[i:i + batch_size]
            print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘: {i+1}~{min(i + batch_size, total_rows)}í–‰")
            
            if isinstance(batch_data[0], (list, tuple)):
                # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                values = []
                for row in batch_data:
                    placeholders = ', '.join(['%s'] * len(row))
                    values.append(cursor.mogrify(f"({placeholders})", row).decode('utf-8'))
                
                if values:
                    # ì»¬ëŸ¼ëª…ì„ ë™ì ìœ¼ë¡œ ìƒì„±
                    column_list = ', '.join(columns)
                    
                    insert_sql = f"""
                        INSERT INTO {table_name} ({column_list}) 
                        VALUES {','.join(values)};
                    """
                    print(f"INSERT SQL ì‹¤í–‰: {len(values)}í–‰")
                    cursor.execute(insert_sql)
            else:
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                values = []
                for row in batch_data:
                    values.append(cursor.mogrify("(%s)", (json.dumps(row, ensure_ascii=False),)).decode('utf-8'))
                
                if values:
                    cursor.execute(f"""
                        INSERT INTO {table_name} (data) VALUES {','.join(values)};
                    """)
            
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, total_rows)}/{total_rows}")
    
    def _clear_postgresql_table(self, conf, table_name):
        """PostgreSQL í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        import psycopg2
        from psycopg2 import OperationalError
        
        try:
            conn = psycopg2.connect(
                host=conf['host'],
                port=conf['port'],
                database=conf['database'],
                user=conf['user'],
                password=conf['password']
            )
            conn.autocommit = False
            
            cursor = conn.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' AND table_name = '{table_name}'
                );
            """)
            table_exists = cursor.fetchone()[0]
            
            if table_exists:
                cursor.execute(f"DELETE FROM {table_name}")
                deleted_count = cursor.rowcount
                conn.commit()
                print(f"PostgreSQL í…Œì´ë¸” {table_name} ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}í–‰")
            else:
                print(f"PostgreSQL í…Œì´ë¸” {table_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            print(f"PostgreSQL í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        finally:
            if 'conn' in locals() and conn:
                conn.close()

    def _save_to_altibase_with_columns(self, conf, table_name, data, columns, sync_mode="full"):
        """Altibaseì— ë°ì´í„° ì €ì¥ (ì»¬ëŸ¼ëª… í¬í•¨) - ë™ê¸°í™” ì „ëµ ì ìš©"""
        import jaydebeapi
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                print(f"Altibase ì €ì¥ ì‹œì‘: {conf['host']}:{conf['port']}/{conf['database']}")
                print(f"í…Œì´ë¸”ëª…: {table_name}, ë°ì´í„° í–‰ ìˆ˜: {len(data)}, ë™ê¸°í™” ëª¨ë“œ: {sync_mode}")
                
                url = f"jdbc:Altibase://{conf['host']}:{conf['port']}/{conf['database']}"
                jar = "/app/db_connection_test/Altibase.jar"
                
                conn = jaydebeapi.connect(
                    "Altibase.jdbc.driver.AltibaseDriver",
                    url,
                    [conf['user'], conf['password']],
                    jar
                )
                conn.autocommit = False  # íŠ¸ëœì­ì…˜ ìˆ˜ë™ ê´€ë¦¬
                
                cursor = conn.jconn.createStatement()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
                cursor.execute(f"""
                    SELECT COUNT(*) FROM SYSTEM_.SYS_TABLES_ WHERE TABLE_NAME = '{table_name}'
                """)
                table_exists = cursor.fetchone()[0] > 0
                print(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
                
                if not table_exists:
                    # í…Œì´ë¸” ìƒì„±
                    if data and len(data) > 0:
                        # ì²« ë²ˆì§¸ í–‰ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„
                        first_row = data[0]
                        if isinstance(first_row, (list, tuple)) and len(first_row) <= 10:
                            # ì»¬ëŸ¼ë³„ ì €ì¥ (ì»¬ëŸ¼ ìˆ˜ê°€ ì ì„ ë•Œë§Œ)
                            columns = []
                            for i, value in enumerate(first_row):
                                if isinstance(value, int):
                                    columns.append(f"col_{i} INTEGER")
                                elif isinstance(value, float):
                                    columns.append(f"col_{i} NUMERIC")
                                elif isinstance(value, bool):
                                    columns.append(f"col_{i} SMALLINT")  # Altibaseì—ì„œ BOOLEAN ëŒ€ì‹  SMALLINT ì‚¬ìš©
                                else:
                                    columns.append(f"col_{i} VARCHAR(1000)")
                            
                            create_sql = f"""
                                CREATE TABLE {table_name} (
                                    id INTEGER PRIMARY KEY,
                                    {', '.join(columns)}
                                );
                            """
                        else:
                            # JSON í˜•íƒœë¡œ ì €ì¥
                            create_sql = f"""
                                CREATE TABLE {table_name} (
                                    id INTEGER PRIMARY KEY,
                                    data VARCHAR(4000)
                                );
                            """
                    else:
                        # ê¸°ë³¸ êµ¬ì¡°
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id INTEGER PRIMARY KEY,
                                data VARCHAR(4000)
                            );
                        """
                    
                    cursor.execute(create_sql)
                    conn.jconn.commit()
                    print(f"í…Œì´ë¸” {table_name}ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ë™ê¸°í™” ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
                if sync_mode == "full":
                    # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…
                    print("ì „ì²´ ë™ê¸°í™” ì „ëµ ì ìš©: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…")
                    
                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                    cursor.execute(f"DELETE FROM {table_name}")
                    deleted_count = cursor.rowcount
                    print(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}í–‰")
                    
                    # ìƒˆ ë°ì´í„° ì‚½ì…
                    if data and len(data) > 0:
                        self._insert_altibase_data(cursor, table_name, data, columns)
                        print(f"Altibase ì „ì²´ ë™ê¸°í™” ì™„ë£Œ: ì‚­ì œ {deleted_count}í–‰, ì‚½ì… {len(data)}í–‰")
                    else:
                        print(f"Altibase ì „ì²´ ë™ê¸°í™” ì™„ë£Œ: ì‚­ì œ {deleted_count}í–‰, ì‚½ì… 0í–‰")
                        
                elif sync_mode == "incremental":
                    # ì¦ë¶„ ë™ê¸°í™”: ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                    print("ì¦ë¶„ ë™ê¸°í™” ì „ëµ ì ìš©: ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€")
                    
                    if data and len(data) > 0:
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„ì‹œ í…Œì´ë¸” ì‚¬ìš©
                        temp_table = f"{table_name}_temp_{int(time.time())}"
                        
                        # ì„ì‹œ í…Œì´ë¸” ìƒì„±
                        cursor.execute(f"CREATE TABLE {temp_table} AS SELECT * FROM {table_name} WHERE 1=0")
                        self._insert_altibase_data(cursor, temp_table, data, columns)
                        
                        # ê¸°ì¡´ í…Œì´ë¸”ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                        if isinstance(data[0], (list, tuple)) and len(columns) > 0:
                            # ì»¬ëŸ¼ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                            merge_sql = f"""
                                INSERT INTO {table_name} ({', '.join(columns)})
                                SELECT {', '.join(columns)} FROM {temp_table}
                                WHERE NOT EXISTS (
                                    SELECT 1 FROM {table_name} t2 
                                    WHERE t2.{columns[0]} = {temp_table}.{columns[0]}
                                );
                            """
                        else:
                            # JSON ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                            merge_sql = f"""
                                INSERT INTO {table_name} (data)
                                SELECT data FROM {temp_table}
                                WHERE NOT EXISTS (
                                    SELECT 1 FROM {table_name} t2 
                                    WHERE t2.data = {temp_table}.data
                                );
                            """
                        
                        cursor.execute(merge_sql)
                        inserted_count = cursor.rowcount
                        
                        # ì„ì‹œ í…Œì´ë¸” ì‚­ì œ
                        cursor.execute(f"DROP TABLE {temp_table}")
                        
                        print(f"Altibase ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ: ì¶”ê°€ {inserted_count}í–‰")
                    else:
                        print("Altibase ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ: ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
                
                conn.jconn.commit()
                return True, None, None
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            import traceback
            error_msg = f"Altibase ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\n{traceback.format_exc()}"
            print(error_msg)
            return False, error_msg, None
    
    def _insert_altibase_data(self, cursor, table_name, data, columns):
        """Altibaseì— ë°ì´í„° ì‚½ì… (í—¬í¼ í•¨ìˆ˜)"""
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        batch_size = 1000
        total_rows = len(data)
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {total_rows}í–‰, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°ì´í„° ì‚½ì… (ë°°ì¹˜ ì²˜ë¦¬)
        for i in range(0, total_rows, batch_size):
            batch_data = data[i:i + batch_size]
            print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘: {i+1}~{min(i + batch_size, total_rows)}í–‰")
            
            for row_id, row in enumerate(batch_data, i + 1):
                # í…Œì´ë¸” êµ¬ì¡°ì— ë”°ë¼ ì ì ˆí•œ INSERT ì‚¬ìš©
                if isinstance(row, (list, tuple)) and len(row) <= 10:
                    # ì»¬ëŸ¼ë³„ ì €ì¥
                    placeholders = ', '.join(['?'] * len(row))
                    column_list = ', '.join(columns)
                    
                    cursor.execute(f"""
                        INSERT INTO {table_name} (id, {column_list}) VALUES (?, {placeholders});
                    """, [row_id] + list(row))
                else:
                    # JSON í˜•íƒœë¡œ ì €ì¥
                    row_data = json.dumps(row, ensure_ascii=False)[:3990]  # Altibase VARCHAR ì œí•œ
                    
                    cursor.execute(f"""
                        INSERT INTO {table_name} (id, data) VALUES (?, ?);
                    """, (row_id, row_data))
            
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, total_rows)}/{total_rows}")
    
    def _clear_altibase_table(self, conf, table_name):
        """Altibase í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        import jaydebeapi
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                url = f"jdbc:Altibase://{conf['host']}:{conf['port']}/{conf['database']}"
                jar = "/app/db_connection_test/Altibase.jar"
                
                conn = jaydebeapi.connect(
                    "Altibase.jdbc.driver.AltibaseDriver",
                    url,
                    [conf['user'], conf['password']],
                    jar
                )
                conn.autocommit = False
                
                cursor = conn.jconn.createStatement()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cursor.execute(f"""
                    SELECT COUNT(*) FROM SYSTEM_.SYS_TABLES_ WHERE TABLE_NAME = '{table_name}'
                """)
                table_exists = cursor.fetchone()[0] > 0
                
                if table_exists:
                    cursor.execute(f"DELETE FROM {table_name}")
                    deleted_count = cursor.rowcount
                    conn.jconn.commit()
                    print(f"Altibase í…Œì´ë¸” {table_name} ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}í–‰")
                else:
                    print(f"Altibase í…Œì´ë¸” {table_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                return True
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            print(f"Altibase í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _save_to_informix_with_columns(self, conf, table_name, data, columns, sync_mode="full"):
        """Informixì— ë°ì´í„° ì €ì¥ (ì»¬ëŸ¼ëª… í¬í•¨) - ë™ê¸°í™” ì „ëµ ì ìš©"""
        import jaydebeapi
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                print(f"Informix ì €ì¥ ì‹œì‘: {conf['host']}:{conf['port']}/{conf['database']}")
                print(f"í…Œì´ë¸”ëª…: {table_name}, ë°ì´í„° í–‰ ìˆ˜: {len(data)}, ë™ê¸°í™” ëª¨ë“œ: {sync_mode}")
                
                url = f"jdbc:informix-sqli://{conf['host']}:{conf['port']}/{conf['database']}:NEWCODESET=EUC-KR,cp1252,819"
                jar = "/app/db_connection_test/ifxjdbc.jar"
                
                conn = jaydebeapi.connect(
                    "com.informix.jdbc.IfxDriver",  # ì˜¬ë°”ë¥¸ ë“œë¼ì´ë²„ í´ë˜ìŠ¤ëª… ì‚¬ìš©
                    url,
                    [conf['user'], conf['password']],
                    jar
                )
                conn.autocommit = False  # íŠ¸ëœì­ì…˜ ìˆ˜ë™ ê´€ë¦¬
                
                cursor = conn.jconn.createStatement()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
                cursor.execute(f"""
                    SELECT COUNT(*) FROM sysmaster:sys_tables WHERE tabname = '{table_name}'
                """)
                table_exists = cursor.fetchone()[0] > 0
                print(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
                
                if not table_exists:
                    # í…Œì´ë¸” ìƒì„±
                    if data and len(data) > 0:
                        # ì²« ë²ˆì§¸ í–‰ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„
                        first_row = data[0]
                        if isinstance(first_row, (list, tuple)):
                            # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                            create_sql = f"""
                                CREATE TABLE {table_name} (
                                    id SERIAL PRIMARY KEY,
                                    {', '.join([f'{col} VARCHAR(1000)' for col in columns])}
                                );
                            """
                        else:
                            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                            create_sql = f"""
                                CREATE TABLE {table_name} (
                                    id SERIAL PRIMARY KEY,
                                    data TEXT
                                );
                            """
                    else:
                        # ê¸°ë³¸ êµ¬ì¡°
                        create_sql = f"""
                            CREATE TABLE {table_name} (
                                id SERIAL PRIMARY KEY,
                                data TEXT
                            );
                        """
                    
                    cursor.execute(create_sql)
                    conn.jconn.commit()
                    print(f"í…Œì´ë¸” {table_name}ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ë™ê¸°í™” ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
                if sync_mode == "full":
                    # ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…
                    print("ì „ì²´ ë™ê¸°í™” ì „ëµ ì ìš©: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì‚½ì…")
                    
                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                    cursor.execute(f"DELETE FROM {table_name}")
                    deleted_count = cursor.rowcount
                    print(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}í–‰")
                    
                    # ìƒˆ ë°ì´í„° ì‚½ì…
                    if data and len(data) > 0:
                        self._insert_informix_data(cursor, table_name, data, columns)
                        print(f"Informix ì „ì²´ ë™ê¸°í™” ì™„ë£Œ: ì‚­ì œ {deleted_count}í–‰, ì‚½ì… {len(data)}í–‰")
                    else:
                        print(f"Informix ì „ì²´ ë™ê¸°í™” ì™„ë£Œ: ì‚­ì œ {deleted_count}í–‰, ì‚½ì… 0í–‰")
                        
                elif sync_mode == "incremental":
                    # ì¦ë¶„ ë™ê¸°í™”: ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                    print("ì¦ë¶„ ë™ê¸°í™” ì „ëµ ì ìš©: ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€")
                    
                    if data and len(data) > 0:
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„ì‹œ í…Œì´ë¸” ì‚¬ìš©
                        temp_table = f"{table_name}_temp_{int(time.time())}"
                        
                        # ì„ì‹œ í…Œì´ë¸” ìƒì„±
                        cursor.execute(f"CREATE TABLE {temp_table} AS SELECT * FROM {table_name} WHERE 1=0")
                        self._insert_informix_data(cursor, temp_table, data, columns)
                        
                        # ê¸°ì¡´ í…Œì´ë¸”ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                        if isinstance(data[0], (list, tuple)) and len(columns) > 0:
                            # ì»¬ëŸ¼ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                            merge_sql = f"""
                                INSERT INTO {table_name} ({', '.join(columns)})
                                SELECT {', '.join(columns)} FROM {temp_table}
                                WHERE NOT EXISTS (
                                    SELECT 1 FROM {table_name} t2 
                                    WHERE t2.{columns[0]} = {temp_table}.{columns[0]}
                                );
                            """
                        else:
                            # JSON ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                            merge_sql = f"""
                                INSERT INTO {table_name} (data)
                                SELECT data FROM {temp_table}
                                WHERE NOT EXISTS (
                                    SELECT 1 FROM {table_name} t2 
                                    WHERE t2.data = {temp_table}.data
                                );
                            """
                        
                        cursor.execute(merge_sql)
                        inserted_count = cursor.rowcount
                        
                        # ì„ì‹œ í…Œì´ë¸” ì‚­ì œ
                        cursor.execute(f"DROP TABLE {temp_table}")
                        
                        print(f"Informix ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ: ì¶”ê°€ {inserted_count}í–‰")
                    else:
                        print("Informix ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ: ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
                    
                conn.jconn.commit()
                return True, None, None
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            import traceback
            error_msg = f"Informix ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\n{traceback.format_exc()}"
            print(error_msg)
            return False, error_msg, None
    
    def _insert_informix_data(self, cursor, table_name, data, columns):
        """Informixì— ë°ì´í„° ì‚½ì… (í—¬í¼ í•¨ìˆ˜)"""
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        batch_size = 1000
        total_rows = len(data)
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {total_rows}í–‰, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°ì´í„° ì‚½ì… (ë°°ì¹˜ ì²˜ë¦¬)
        for i in range(0, total_rows, batch_size):
            batch_data = data[i:i + batch_size]
            print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘: {i+1}~{min(i + batch_size, total_rows)}í–‰")
            
            for row in batch_data:
                if isinstance(row, (list, tuple)):
                    # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                    placeholders = ', '.join(['?'] * len(row))
                    column_list = ', '.join(columns)
                    
                    cursor.execute(f"""
                        INSERT INTO {table_name} ({column_list}) VALUES ({placeholders});
                    """, list(row))
                else:
                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                    row_data = json.dumps(row, ensure_ascii=False)
                    cursor.execute(f"""
                        INSERT INTO {table_name} (data) VALUES (?);
                    """, (row_data,))
            
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, total_rows)}/{total_rows}")
    
    def _clear_informix_table(self, conf, table_name):
        """Informix í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        import jaydebeapi
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
            original_cwd = os.getcwd()
            os.chdir('/app/db_connection_test')
            
            try:
                url = f"jdbc:informix-sqli://{conf['host']}:{conf['port']}/{conf['database']}:NEWCODESET=EUC-KR,cp1252,819"
                jar = "/app/db_connection_test/ifxjdbc.jar"
                
                conn = jaydebeapi.connect(
                    "com.informix.jdbc.IfxDriver",
                    url,
                    [conf['user'], conf['password']],
                    jar
                )
                conn.autocommit = False
                
                cursor = conn.jconn.createStatement()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cursor.execute(f"""
                    SELECT COUNT(*) FROM sysmaster:sys_tables WHERE tabname = '{table_name}'
                """)
                table_exists = cursor.fetchone()[0] > 0
                
                if table_exists:
                    cursor.execute(f"DELETE FROM {table_name}")
                    deleted_count = cursor.rowcount
                    conn.jconn.commit()
                    print(f"Informix í…Œì´ë¸” {table_name} ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {deleted_count}í–‰")
                else:
                    print(f"Informix í…Œì´ë¸” {table_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                return True
                
            finally:
                # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
                os.chdir(original_cwd)
                
        except Exception as e:
            print(f"Informix í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

# ë¼ìš°íŠ¸
@app.context_processor
def inject_kst_functions():
    """í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©í•  KST ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ì£¼ì…"""
    return {
        'format_kst_time': format_kst_time,
        'format_db_kst_time': format_db_kst_time,
        'format_scheduler_time': format_scheduler_time,
        'get_kst_now': get_kst_now,
        'utc_to_kst': utc_to_kst,
        'parse_cron_expression': parse_cron_expression
    }

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return redirect(url_for('dashboard'))

@app.route('/dashboard')
def dashboard():
    """ëŒ€ì‹œë³´ë“œ"""
    # í†µê³„ ì •ë³´
    total_jobs = db.session.query(BatchJob).count()
    active_jobs = db.session.query(BatchJob).filter_by(is_active=True).count()
    total_schedules = db.session.query(BatchSchedule).count()
    active_schedules = db.session.query(BatchSchedule).filter_by(is_active=True).count()
    
    # ìµœê·¼ ë¡œê·¸
    recent_logs = db.session.query(BatchLog).order_by(BatchLog.started_at.desc()).limit(10).all()
    
    return render_template('dashboard.html', 
                         total_jobs=total_jobs,
                         active_jobs=active_jobs,
                         total_schedules=total_schedules,
                         active_schedules=active_schedules,
                         recent_logs=recent_logs)

@app.route('/servers')
def servers():
    """ì„œë²„ ê´€ë¦¬ í˜ì´ì§€"""
    servers = db.session.query(ServerConfig).all()
    return render_template('servers.html', servers=servers)

@app.route('/servers/add', methods=['GET', 'POST'])
def add_server():
    """ì„œë²„ ì¶”ê°€"""
    if request.method == 'POST':
        data = request.form
        
        server = ServerConfig(
            name=data['name'],
            type=data['type'],
            host=data['host'],
            port=int(data['port']),
            database=data['database'],
            user=data['user'],
            password=data['password']
        )
        
        db.session.add(server)
        db.session.commit()
        
        # ì„¤ì • íŒŒì¼ ë™ê¸°í™”
        config_manager = ConfigManager()
        config_manager.sync_from_database()
        
        flash('ì„œë²„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
        return redirect(url_for('servers'))
    
    return render_template('add_server.html')

@app.route('/servers/<int:server_id>/edit', methods=['GET', 'POST'])
def edit_server(server_id):
    """ì„œë²„ ìˆ˜ì •"""
    server = db.session.query(ServerConfig).get_or_404(server_id)
    
    if request.method == 'POST':
        data = request.form
        
        server.name = data['name']
        server.type = data['type']
        server.host = data['host']
        server.port = int(data['port'])
        server.database = data['database']
        server.user = data['user']
        server.password = data['password']
        
        db.session.commit()
        
        # ì„¤ì • íŒŒì¼ ë™ê¸°í™”
        config_manager = ConfigManager()
        config_manager.sync_from_database()
        
        flash('ì„œë²„ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
        return redirect(url_for('servers'))
    
    return render_template('edit_server.html', server=server)

@app.route('/servers/<int:server_id>/delete', methods=['POST'])
def delete_server(server_id):
    """ì„œë²„ ì‚­ì œ"""
    server = db.session.query(ServerConfig).get_or_404(server_id)
    db.session.delete(server)
    db.session.commit()
    
    # ì„¤ì • íŒŒì¼ ë™ê¸°í™”
    config_manager = ConfigManager()
    config_manager.sync_from_database()
    
    flash('ì„œë²„ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
    return redirect(url_for('servers'))

@app.route('/query')
def query():
    """ì¿¼ë¦¬ ì‹¤í–‰ í˜ì´ì§€"""
    servers = db.session.query(ServerConfig).all()
    return render_template('query.html', servers=servers)

@app.route('/query/execute', methods=['POST'])
def execute_query():
    """ì¿¼ë¦¬ ì‹¤í–‰"""
    data = request.json
    
    try:
        # ì„¤ì • íŒŒì¼ ë™ê¸°í™”
        config_manager = ConfigManager()
        config_manager.sync_from_database()
        
        # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
        original_cwd = os.getcwd()
        os.chdir('/app/db_connection_test')
        
        try:
            # ì¿¼ë¦¬ ì‹¤í–‰ (ì›ë˜ db_query.pyì˜ execute_query ì‚¬ìš©)
            start_time = time.time()
            result, success, error = db_execute_query(data['server'], data['query'])
            end_time = time.time()
            
            if not success:
                raise Exception(f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {error}")
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            history = QueryHistory(
                server_name=data['server'],
                query=data['query'],
                result_count=len(result) if result else 0,
                execution_time=end_time - start_time,
                status='success'
            )
            db.session.add(history)
            db.session.commit()
            
            return jsonify({
                'success': True,
                'result': {
                    'columns': [],  # ì›ë˜ execute_queryëŠ” ì»¬ëŸ¼ëª…ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
                    'rows': result,
                    'row_count': len(result) if result else 0
                }
            })
        finally:
            # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
            os.chdir(original_cwd)
        
    except Exception as e:
        # ì—ëŸ¬ íˆìŠ¤í† ë¦¬ ì €ì¥
        history = QueryHistory(
            server_name=data['server'],
            query=data['query'],
            status='failed',
            error_message=str(e)
        )
        db.session.add(history)
        db.session.commit()
        
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/jobs')
def jobs():
    """ë°°ì¹˜ ì‘ì—… ê´€ë¦¬ í˜ì´ì§€"""
    jobs = db.session.query(BatchJob).all()
    return render_template('jobs.html', jobs=jobs)

@app.route('/jobs/add', methods=['GET', 'POST'])
def add_job():
    """ë°°ì¹˜ ì‘ì—… ì¶”ê°€"""
    if request.method == 'POST':
        data = request.form
        
        job = BatchJob(
            name=data['name'],
            description=data['description'],
            source_server=data['source_server'],
            query=data['query'],
            target_server=data['target_server'],
            target_table=data['target_table'],
            chunk_size=int(data['chunk_size']),
            num_workers=int(data['num_workers']),
            is_active='is_active' in data,
            # ì¦ë¶„ ë™ê¸°í™” ê´€ë ¨ í•„ë“œ ì¶”ê°€
            incremental_sync='incremental_sync' in data,
            sync_key_column=data.get('sync_key_column', ''),
            last_sync_value=data.get('last_sync_value', ''),
            sync_strategy=data.get('sync_strategy', 'timestamp')
        )
        
        db.session.add(job)
        db.session.commit()
        
        flash('ë°°ì¹˜ ì‘ì—…ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
        return redirect(url_for('jobs'))
    
    servers = db.session.query(ServerConfig).all()
    return render_template('add_job.html', servers=servers)

@app.route('/jobs/<int:job_id>/edit', methods=['GET', 'POST'])
def edit_job(job_id):
    """ë°°ì¹˜ ì‘ì—… ìˆ˜ì •"""
    job = db.session.get(BatchJob, job_id)
    if not job:
        abort(404)
    
    if request.method == 'POST':
        data = request.form
        
        job.name = data['name']
        job.description = data['description']
        job.source_server = data['source_server']
        job.query = data['query']
        job.target_server = data['target_server']
        job.target_table = data['target_table']
        job.chunk_size = int(data['chunk_size'])
        job.num_workers = int(data['num_workers'])
        job.is_active = 'is_active' in data
        
        # ì¦ë¶„ ë™ê¸°í™” ê´€ë ¨ í•„ë“œ ì¶”ê°€
        job.incremental_sync = 'incremental_sync' in data
        job.sync_key_column = data.get('sync_key_column', '')
        job.last_sync_value = data.get('last_sync_value', '')
        job.sync_strategy = data.get('sync_strategy', 'timestamp')
        
        db.session.commit()
        
        flash('ë°°ì¹˜ ì‘ì—…ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
        return redirect(url_for('jobs'))
    
    servers = db.session.query(ServerConfig).all()
    return render_template('edit_job.html', job=job, servers=servers)

@app.route('/jobs/<int:job_id>/delete', methods=['POST'])
def delete_job(job_id):
    """ë°°ì¹˜ ì‘ì—… ì‚­ì œ"""
    job = db.session.get(BatchJob, job_id)
    if not job:
        abort(404)
    
    db.session.delete(job)
    db.session.commit()
    
    flash('ë°°ì¹˜ ì‘ì—…ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
    return redirect(url_for('jobs'))

@app.route('/jobs/<int:job_id>/execute', methods=['POST'])
def execute_job(job_id):
    """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰"""
    job = db.session.get(BatchJob, job_id)
    if not job:
        abort(404)
    
    executor = BatchExecutor()
    success = executor.execute_job(job_id)
    
    if success:
        flash('ë°°ì¹˜ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
    else:
        flash('ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'error')
    
    return redirect(url_for('jobs'))

@app.route('/schedules')
def schedules():
    """ìŠ¤ì¼€ì¤„ ê´€ë¦¬ í˜ì´ì§€"""
    schedules = db.session.query(BatchSchedule).all()
    jobs = db.session.query(BatchJob).filter_by(is_active=True).all()
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ ìƒíƒœ í™•ì¸
    scheduler_jobs = scheduler.get_jobs()
    scheduler_job_ids = {job.id for job in scheduler_jobs}
    
    for schedule in schedules:
        schedule.scheduler_registered = f'schedule_{schedule.id}' in scheduler_job_ids
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ì‹¤ì œ next_run_time ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ KST)
        scheduler_next_run = get_scheduler_next_run_time(schedule.id)
        if scheduler_next_run:
            schedule.scheduler_next_run = format_scheduler_time(scheduler_next_run, '%m-%d %H:%M')
            schedule.scheduler_next_run_year = format_scheduler_time(scheduler_next_run, '%Y')
        else:
            # ìŠ¤ì¼€ì¤„ëŸ¬ì— ë“±ë¡ë˜ì§€ ì•Šì€ ê²½ìš° DBì˜ ì‹œê°„ ì‚¬ìš© (ì´ë¯¸ KST)
            if schedule.next_run:
                schedule.scheduler_next_run = format_db_kst_time(schedule.next_run, '%m-%d %H:%M')
                schedule.scheduler_next_run_year = format_db_kst_time(schedule.next_run, '%Y')
            else:
                schedule.scheduler_next_run = None
                schedule.scheduler_next_run_year = None
    
    return render_template('schedules.html', schedules=schedules, jobs=jobs)

@app.route('/schedules/add', methods=['GET', 'POST'])
def add_schedule():
    """ìŠ¤ì¼€ì¤„ ì¶”ê°€"""
    if request.method == 'POST':
        data = request.form
        
        print(f"=== ìŠ¤ì¼€ì¤„ ì¶”ê°€ ìš”ì²­ ===")
        print(f"job_id: {data.get('job_id')}")
        print(f"cron_expression: {data.get('cron_expression')}")
        
        schedule = BatchSchedule(
            job_id=int(data['job_id']),
            cron_expression=data['cron_expression'],
            is_active=True  # ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”
        )
        
        db.session.add(schedule)
        db.session.commit()
        
        print(f"ìŠ¤ì¼€ì¤„ ID: {schedule.id}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ì— ì‘ì—… ì¶”ê°€
        job = db.session.query(BatchJob).get(schedule.job_id)
        if job:
            try:
                # ê¸°ì¡´ ì‘ì—…ì´ ìˆë‹¤ë©´ ì œê±°
                try:
                    scheduler.remove_job(f'schedule_{schedule.id}')
                except:
                    pass
                
                # ìƒˆ ì‘ì—… ì¶”ê°€
                scheduler.add_job(
                    func=execute_scheduled_job,
                    trigger=CronTrigger.from_crontab(schedule.cron_expression),
                    args=[schedule.id],
                    id=f'schedule_{schedule.id}',
                    replace_existing=True
                )
                
                # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ì„ DBì— ì €ì¥ (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ KST)
                scheduler_next_run = get_scheduler_next_run_time(schedule.id)
                if scheduler_next_run:
                    # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ Asia/Seoul ì‹œê°„ëŒ€ì´ë¯€ë¡œ ì§ì ‘ ì €ì¥
                    schedule.next_run = scheduler_next_run
                    db.session.commit()
                
                print(f"ìŠ¤ì¼€ì¤„ëŸ¬ì— ì‘ì—… ë“±ë¡ ì„±ê³µ: schedule_{schedule.id}")
                print(f"ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„: {schedule.next_run}")
                
            except Exception as e:
                print(f"ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ ì‹¤íŒ¨: {e}")
                # ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ë¹„í™œì„±í™”
                schedule.is_active = False
                db.session.commit()
                flash(f'ìŠ¤ì¼€ì¤„ì´ ì¶”ê°€ë˜ì—ˆì§€ë§Œ ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}', 'warning')
                return redirect(url_for('schedules'))
        else:
            print(f"ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {schedule.job_id}")
            flash('ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'error')
            return redirect(url_for('schedules'))
        
        flash('ìŠ¤ì¼€ì¤„ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
        return redirect(url_for('schedules'))
    
    # GET ìš”ì²­ì€ schedules í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (í¼ì´ schedules.htmlì— ìˆìŒ)
    return redirect(url_for('schedules'))

@app.route('/schedules/<int:schedule_id>/edit', methods=['GET', 'POST'])
def edit_schedule(schedule_id):
    """ìŠ¤ì¼€ì¤„ ìˆ˜ì •"""
    schedule = db.session.query(BatchSchedule).get_or_404(schedule_id)
    
    if request.method == 'POST':
        data = request.form
        
        print(f"=== ìŠ¤ì¼€ì¤„ ìˆ˜ì • ìš”ì²­ ===")
        print(f"schedule_id: {schedule_id}")
        print(f"cron_expression: {data.get('cron_expression')}")
        print(f"is_active: {'is_active' in data}")
        
        schedule.cron_expression = data['cron_expression']
        schedule.is_active = 'is_active' in data
        
        db.session.commit()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        try:
            # ê¸°ì¡´ ì‘ì—… ì œê±°
            try:
                scheduler.remove_job(f'schedule_{schedule.id}')
            except:
                pass
            
            if schedule.is_active:
                # ìƒˆ ì‘ì—… ì¶”ê°€
                scheduler.add_job(
                    func=execute_scheduled_job,
                    trigger=CronTrigger.from_crontab(schedule.cron_expression),
                    args=[schedule.id],
                    id=f'schedule_{schedule.id}',
                    replace_existing=True
                )
                
                # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
                try:
                    scheduler_next_run = get_scheduler_next_run_time(schedule.id)
                    if scheduler_next_run:
                        # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ Asia/Seoul ì‹œê°„ëŒ€ì´ë¯€ë¡œ ì§ì ‘ ì €ì¥
                        schedule.next_run = scheduler_next_run
                except:
                    pass
                
                print(f"ìŠ¤ì¼€ì¤„ëŸ¬ì— ì‘ì—… ë“±ë¡ ì„±ê³µ: schedule_{schedule.id}")
                print(f"ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„: {schedule.next_run}")
            else:
                # ë¹„í™œì„±í™” ì‹œ next_run ì´ˆê¸°í™”
                schedule.next_run = None
                db.session.commit()
                print(f"ìŠ¤ì¼€ì¤„ ë¹„í™œì„±í™”: schedule_{schedule.id}")
                
        except Exception as e:
            print(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ë¹„í™œì„±í™”
            schedule.is_active = False
            schedule.next_run = None
            db.session.commit()
            flash(f'ìŠ¤ì¼€ì¤„ì´ ìˆ˜ì •ë˜ì—ˆì§€ë§Œ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}', 'warning')
            return redirect(url_for('schedules'))
        
        flash('ìŠ¤ì¼€ì¤„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
        return redirect(url_for('schedules'))
    
    # GET ìš”ì²­ì€ schedules í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (í¼ì´ schedules.htmlì— ìˆìŒ)
    return redirect(url_for('schedules'))

@app.route('/schedules/<int:schedule_id>/delete', methods=['POST'])
def delete_schedule(schedule_id):
    """ìŠ¤ì¼€ì¤„ ì‚­ì œ"""
    schedule = db.session.query(BatchSchedule).get_or_404(schedule_id)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì œê±°
    scheduler.remove_job(f'schedule_{schedule.id}')
    
    db.session.delete(schedule)
    db.session.commit()
    
    flash('ìŠ¤ì¼€ì¤„ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'success')
    return redirect(url_for('schedules'))

def execute_scheduled_job(schedule_id):
    """ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰"""
    with app.app_context():
        try:
            schedule = db.session.query(BatchSchedule).get(schedule_id)
            if not schedule:
                print(f"âŒ ìŠ¤ì¼€ì¤„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: schedule_id={schedule_id}")
                return
            
            if not schedule.is_active:
                print(f"âš ï¸ ìŠ¤ì¼€ì¤„ì´ ë¹„í™œì„±í™”ë¨: schedule_id={schedule_id}")
                return
            
            print(f"ğŸ”„ ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ì‹œì‘: schedule_id={schedule_id}, job_id={schedule.job_id}")
            
            # ì‹¤í–‰ ë¡œê·¸ ìƒì„± - ëª…ì‹œì ìœ¼ë¡œ KST ì‹œê°„ ì‚¬ìš©
            started_at = get_kst_now()
            log = BatchLog(
                job_id=schedule.job_id,
                schedule_id=schedule.id,
                status='running',
                started_at=started_at
            )
            db.session.add(log)
            db.session.commit()
            
            # ì‘ì—… ì‹¤í–‰
            executor = BatchExecutor()
            success = executor.execute_job(schedule.job_id)
            
            # ë¡œê·¸ ì—…ë°ì´íŠ¸ - ëª…ì‹œì ìœ¼ë¡œ KST ì‹œê°„ ì‚¬ìš©
            completed_at = get_kst_now()
            log.completed_at = completed_at
            log.status = 'success' if success else 'failed'
            
            # ì‹œê°„ëŒ€ ì•ˆì „í•œ ì†Œìš” ì‹œê°„ ê³„ì‚°
            try:
                # ë‘ ì‹œê°„ ëª¨ë‘ KST ì‹œê°„ëŒ€ì´ë¯€ë¡œ ì§ì ‘ ê³„ì‚° ê°€ëŠ¥
                log.duration_seconds = (completed_at - started_at).total_seconds()
            except Exception as e:
                print(f"âš ï¸ ì†Œìš” ì‹œê°„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                log.duration_seconds = None
            
            # ìŠ¤ì¼€ì¤„ ì •ë³´ ì—…ë°ì´íŠ¸ - ëª…ì‹œì ìœ¼ë¡œ KST ì‹œê°„ ì‚¬ìš©
            schedule.last_run = get_kst_now()
            
            # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
            try:
                scheduler_job = scheduler.get_job(f'schedule_{schedule.id}')
                if scheduler_job and scheduler_job.next_run_time:
                    # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ Asia/Seoul ì‹œê°„ëŒ€ì´ë¯€ë¡œ ì§ì ‘ ì €ì¥
                    schedule.next_run = scheduler_job.next_run_time
            except Exception as e:
                print(f"âš ï¸ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            
            db.session.commit()
            
            print(f"âœ… ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ì™„ë£Œ: schedule_id={schedule_id}, success={success}")
            
        except Exception as e:
            print(f"âŒ ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: schedule_id={schedule_id}, error={e}")
            
            # ì˜¤ë¥˜ ë¡œê·¸ ê¸°ë¡ - ëª…ì‹œì ìœ¼ë¡œ KST ì‹œê°„ ì‚¬ìš©
            try:
                log = BatchLog(
                    job_id=schedule.job_id if 'schedule' in locals() else None,
                    schedule_id=schedule_id,
                    status='failed',
                    started_at=get_kst_now(),
                    completed_at=get_kst_now(),
                    error_message=str(e)
                )
                db.session.add(log)
                db.session.commit()
            except:
                pass

@app.route('/logs')
def logs():
    """ë¡œê·¸ ì¡°íšŒ í˜ì´ì§€"""
    page = request.args.get('page', 1, type=int)
    logs = db.session.query(BatchLog).order_by(BatchLog.started_at.desc()).paginate(
        page=page, per_page=20, error_out=False
    )
    return render_template('logs.html', logs=logs)

@app.route('/api/test-connection/<server_name>', methods=['POST'])
def test_connection(server_name):
    """ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = "/app/logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file_path = os.path.join(log_dir, "db_connection.log")
        
        # ë¡œê·¸ ê¸°ë¡
        import logging
        logging.basicConfig(
            filename=log_file_path,
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            encoding='utf-8'
        )
        
        # ì„¤ì • íŒŒì¼ ë™ê¸°í™”
        config_manager = ConfigManager()
        config_manager.sync_from_database()
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸ (ì›ë˜ db_query.pyì˜ í•¨ìˆ˜ë“¤ ì‚¬ìš©)
        logging.info(f"=== {server_name} ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ db_connection_testë¡œ ë³€ê²½
        original_cwd = os.getcwd()
        os.chdir('/app/db_connection_test')
        
        try:
            conf = get_server_config(server_name)
            db_type = conf['type']
            
            logging.info(f"ì„œë²„ ì •ë³´: {db_type}://{conf['host']}:{conf['port']}/{conf['database']}")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            if db_type == 'postgresql':
                test_query = "SELECT 1 as test"
            elif db_type == 'altibase':
                test_query = "SELECT 1 FROM DUAL"
            elif db_type == 'informix':
                test_query = "SELECT 1 FROM DUAL" # InformixëŠ” ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
            else:
                test_query = "SELECT 1 FROM DUAL" # ê¸°ë³¸ ì¿¼ë¦¬
            
            logging.info(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
            
            if db_type == 'altibase':
                result, success, error = test_altibase_connection(conf, test_query)
            elif db_type == 'informix':
                result, success, error = test_informix_connection(conf, test_query)
            else:
                result, success, error = db_execute_query(server_name, test_query)
            
            if success:
                logging.info(f"âœ… {server_name} ({db_type}) ì—°ê²° ì„±ê³µ")
                logging.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}")
                return jsonify({
                    "success": True,
                    "message": f"{db_type} ì—°ê²° ì„±ê³µ",
                    "test_result": result
                })
            else:
                logging.error(f"âŒ {server_name} ({db_type}) ì—°ê²° ì‹¤íŒ¨")
                logging.error(f"ì˜¤ë¥˜: {error}")
                return jsonify({
                    "success": False,
                    "error": error
                })
        finally:
            # ì‘ì—… ë””ë ‰í† ë¦¬ ë³µì›
            os.chdir(original_cwd)
        
    except Exception as e:
        logging.error(f"âŒ {server_name} ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return jsonify({"success": False, "error": str(e)})

@app.route('/api/logs')
def api_logs():
    """ë¡œê·¸ API"""
    page = request.args.get('page', 1, type=int)
    logs = db.session.query(BatchLog).order_by(BatchLog.started_at.desc()).paginate(
        page=page, per_page=20, error_out=False
    )
    
    return jsonify({
        'logs': [{
            'id': log.id,
            'job_name': log.job.name,
            'status': log.status,
            'total_rows': log.total_rows,
            'duration_seconds': log.duration_seconds,
            'started_at': format_kst_time(log.started_at) if log.started_at else None,
            'completed_at': format_kst_time(log.completed_at) if log.completed_at else None,
            'error_message': log.error_message
        } for log in logs.items],
        'total': logs.total,
        'pages': logs.pages,
        'current_page': logs.page
    })

@app.route('/api/db-logs')
def api_db_logs():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¡œê·¸ API"""
    try:
        log_file_path = "/app/logs/db_connection.log"
        if os.path.exists(log_file_path):
            with open(log_file_path, 'r', encoding='utf-8') as f:
                # ë§ˆì§€ë§‰ 50ì¤„ë§Œ ì½ê¸°
                lines = f.readlines()
                recent_logs = lines[-50:] if len(lines) > 50 else lines
                return jsonify({
                    'success': True,
                    'logs': recent_logs,
                    'total_lines': len(lines)
                })
        else:
            return jsonify({
                'success': False,
                'error': 'ë¡œê·¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/diagnose-jar/<jar_name>')
def diagnose_jar(jar_name):
    """JAR íŒŒì¼ ì§„ë‹¨ API (ì›ë˜ ê¸°ëŠ¥ì—ëŠ” ì—†ìŒ)"""
    return jsonify({
        'success': False,
        'error': 'JAR ì§„ë‹¨ ê¸°ëŠ¥ì€ ì›ë˜ db_query.pyì— ì—†ë° ê¸°ëŠ¥ì…ë‹ˆë‹¤.'
    })

@app.route('/api/scheduler-status')
def api_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸ API"""
    try:
        jobs = scheduler.get_jobs()
        job_list = []
        
        for job in jobs:
            # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ì„ KSTë¡œ ë³€í™˜ (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ KST)
            next_run_kst = None
            if job.next_run_time:
                next_run_kst = format_scheduler_time(job.next_run_time, '%Y-%m-%d %H:%M:%S')
            
            job_info = {
                'id': job.id,
                'name': job.name,
                'func': str(job.func),
                'trigger': str(job.trigger),
                'next_run_time': next_run_kst,
                'next_run_original': str(job.next_run_time) if job.next_run_time else None,  # ë””ë²„ê¹…ìš©
                'next_run_relative': _get_relative_time(job.next_run_time) if job.next_run_time else None
            }
            job_list.append(job_info)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤ì¼€ì¤„ ì •ë³´ì™€ ë§¤ì¹­
        db_schedules = db.session.query(BatchSchedule).all()
        schedule_status = []
        
        for db_schedule in db_schedules:
            # ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í•´ë‹¹ ì‘ì—… ì°¾ê¸°
            scheduler_job = None
            for job in jobs:
                if job.id == f'schedule_{db_schedule.id}':
                    scheduler_job = job
                    break
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ì„ KSTë¡œ ë³€í™˜ (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ KST)
            next_run_from_scheduler = None
            if scheduler_job and scheduler_job.next_run_time:
                next_run_from_scheduler = format_scheduler_time(scheduler_job.next_run_time, '%Y-%m-%d %H:%M:%S')
            
            schedule_info = {
                'id': db_schedule.id,
                'job_id': db_schedule.job_id,
                'job_name': db_schedule.job.name if db_schedule.job else 'Unknown',
                'cron_expression': db_schedule.cron_expression,
                'is_active': db_schedule.is_active,
                'last_run': format_kst_time(db_schedule.last_run) if db_schedule.last_run else None,
                'created_at': format_kst_time(db_schedule.created_at),
                'scheduler_registered': scheduler_job is not None,
                'next_run_from_db': format_db_kst_time(db_schedule.next_run, '%Y-%m-%d %H:%M:%S') if db_schedule.next_run else None,  # DBëŠ” ì´ë¯¸ KST
                'next_run_from_scheduler': next_run_from_scheduler,
                'next_run_original': str(scheduler_job.next_run_time) if scheduler_job and scheduler_job.next_run_time else None,
                # ìŠ¤ì¼€ì¤„ ëª©ë¡ì—ì„œ ì‚¬ìš©í•  ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ (ìŠ¤ì¼€ì¤„ëŸ¬ ìš°ì„ , DB ë°±ì—…)
                'next_run_display': next_run_from_scheduler or (format_db_kst_time(db_schedule.next_run, '%Y-%m-%d %H:%M:%S') if db_schedule.next_run else None)
            }
            schedule_status.append(schedule_info)
        
        return jsonify({
            'success': True,
            'scheduler_running': scheduler.running,
            'total_jobs': len(jobs),
            'total_schedules': len(db_schedules),
            'active_schedules': len([s for s in db_schedules if s.is_active]),
            'jobs': job_list,
            'schedules': schedule_status,
            'current_time': format_kst_time(get_kst_now())
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

def _get_relative_time(dt):
    """ìƒëŒ€ì  ì‹œê°„ í‘œì‹œ (ì˜ˆ: 5ë¶„ í›„, 2ì‹œê°„ í›„)"""
    if not dt:
        return None
    
    now = get_kst_now()
    # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ KST ì‹œê°„ëŒ€ì´ë¯€ë¡œ tzinfoê°€ ì—†ìœ¼ë©´ KSTë¡œ ì„¤ì •
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST)
    
    diff = dt - now
    
    if diff.total_seconds() < 0:
        return "ì´ë¯¸ ì§€ë‚¨"
    
    days = diff.days
    hours = diff.seconds // 3600
    minutes = (diff.seconds % 3600) // 60
    
    if days > 0:
        return f"{days}ì¼ {hours}ì‹œê°„ í›„"
    elif hours > 0:
        return f"{hours}ì‹œê°„ {minutes}ë¶„ í›„"
    elif minutes > 0:
        return f"{minutes}ë¶„ í›„"
    else:
        return "ê³§ ì‹¤í–‰"

@app.route('/api/test-schedule/<int:schedule_id>')
def api_test_schedule(schedule_id):
    """ìŠ¤ì¼€ì¤„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ API"""
    try:
        schedule = db.session.query(BatchSchedule).get_or_404(schedule_id)
        
        # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰
        executor = BatchExecutor()
        success = executor.execute_job(schedule.job_id)
        
        # ìŠ¤ì¼€ì¤„ ì •ë³´ ì—…ë°ì´íŠ¸
        schedule.last_run = get_kst_now()
        
        # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        try:
            scheduler_job = scheduler.get_job(f'schedule_{schedule.id}')
            if scheduler_job and scheduler_job.next_run_time:
                schedule.next_run = scheduler_job.next_run_time
        except:
            pass
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': 'ìŠ¤ì¼€ì¤„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ',
            'schedule_id': schedule_id,
            'job_id': schedule.job_id,
            'execution_success': success
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/restore-schedules', methods=['POST'])
def api_restore_schedules():
    """ìŠ¤ì¼€ì¤„ ë³µì› API"""
    try:
        restored_count, failed_count = restore_schedules_from_database()
        
        return jsonify({
            'success': True,
            'message': f'ìŠ¤ì¼€ì¤„ ë³µì› ì™„ë£Œ: ì„±ê³µ {restored_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ',
            'restored_count': restored_count,
            'failed_count': failed_count
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

def get_scheduler_next_run_time(schedule_id):
    """ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ì„ KSTë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        scheduler_job = scheduler.get_job(f'schedule_{schedule_id}')
        if scheduler_job and scheduler_job.next_run_time:
            # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ë¯¸ Asia/Seoul ì‹œê°„ëŒ€ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©
            return scheduler_job.next_run_time
        return None
    except Exception as e:
        print(f"ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

def format_scheduler_time(dt, format_str='%Y-%m-%d %H:%M:%S'):
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œê°„ì„ í¬ë§·íŒ… (ì´ë¯¸ KSTì´ë¯€ë¡œ ë³€í™˜ ì—†ì´ í¬ë§·íŒ…ë§Œ)"""
    if dt is None:
        return '-'
    return dt.strftime(format_str)

def restore_schedules_from_database():
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ í™œì„± ìŠ¤ì¼€ì¤„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ì— ë³µì›"""
    try:
        print("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìŠ¤ì¼€ì¤„ ë³µì› ì¤‘...")
        
        # í™œì„± ìŠ¤ì¼€ì¤„ ì¡°íšŒ
        active_schedules = db.session.query(BatchSchedule).filter_by(is_active=True).all()
        
        restored_count = 0
        failed_count = 0
        
        for schedule in active_schedules:
            try:
                # ê¸°ì¡´ ì‘ì—…ì´ ìˆë‹¤ë©´ ì œê±°
                try:
                    scheduler.remove_job(f'schedule_{schedule.id}')
                except:
                    pass
                
                # ìƒˆ ì‘ì—… ì¶”ê°€
                scheduler.add_job(
                    func=execute_scheduled_job,
                    trigger=CronTrigger.from_crontab(schedule.cron_expression),
                    args=[schedule.id],
                    id=f'schedule_{schedule.id}',
                    replace_existing=True
                )
                
                # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸ - ëª…ì‹œì ìœ¼ë¡œ KST ì‹œê°„ ì‚¬ìš©
                scheduler_next_run = get_scheduler_next_run_time(schedule.id)
                if scheduler_next_run:
                    # ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ê°€ì ¸ì˜¨ ì‹œê°„ì´ ì´ë¯¸ KSTì´ë¯€ë¡œ ì§ì ‘ ì €ì¥
                    schedule.next_run = scheduler_next_run
                
                restored_count += 1
                print(f"âœ… ìŠ¤ì¼€ì¤„ ë³µì› ì„±ê³µ: schedule_{schedule.id} ({schedule.cron_expression})")
                
            except Exception as e:
                failed_count += 1
                print(f"âŒ ìŠ¤ì¼€ì¤„ ë³µì› ì‹¤íŒ¨: schedule_{schedule.id} - {e}")
                # ë³µì› ì‹¤íŒ¨ ì‹œ ë¹„í™œì„±í™”
                schedule.is_active = False
                schedule.next_run = None
        
        # ë³€ê²½ì‚¬í•­ ì €ì¥
        db.session.commit()
        
        print(f"ğŸ“Š ìŠ¤ì¼€ì¤„ ë³µì› ì™„ë£Œ: ì„±ê³µ {restored_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")
        return restored_count, failed_count
        
    except Exception as e:
        print(f"âŒ ìŠ¤ì¼€ì¤„ ë³µì› ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
        db.session.rollback()
        return 0, 0

# ì´ˆê¸°í™” í•¨ìˆ˜
def init_app():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”"""
    with app.app_context():
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        db.create_all()
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë™ê¸°í™”
        config_manager = ConfigManager()
        config_manager.sync_to_database()
        
        # ê¸°ë³¸ ì„œë²„ ì„¤ì • (ë¡œì»¬ PostgreSQL)
        if not db.session.query(ServerConfig).filter_by(name='local_pgsql').first():
            local_server = ServerConfig(
                name='local_pgsql',
                type='postgresql',
                host='postgres',  # Docker ì»¨í…Œì´ë„ˆ ì´ë¦„
                port=5432,
                database='fs_master_web',
                user='postgres',
                password=os.environ.get('POSTGRES_PASSWORD')
            )
            db.session.add(local_server)
            db.session.commit()
        
        # ë°ì´í„°ë² ì´ìŠ¤ì˜ í™œì„± ìŠ¤ì¼€ì¤„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ì— ë³µì›
        try:
            print("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìŠ¤ì¼€ì¤„ ë³µì› ì¤‘...")
            restored_count, failed_count = restore_schedules_from_database()
            print(f"âœ… ìŠ¤ì¼€ì¤„ ë³µì› ì™„ë£Œ: ì„±ê³µ {restored_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ìŠ¤ì¼€ì¤„ ë³µì› ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

def setup_only():
    """ì„¤ì • ì „ìš© ëª¨ë“œ"""
    print("ğŸ—ƒï¸ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì¤‘...")
    init_app()
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='FS Master Web Application')
    parser.add_argument('--setup-only', action='store_true', 
                       help='ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ë§Œ ìƒì„±í•˜ê³  ì¢…ë£Œ')
    parser.add_argument('--host', default='0.0.0.0', 
                       help='ì„œë²„ í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 0.0.0.0)')
    parser.add_argument('--port', type=int, default=5000, 
                       help='ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 5000)')
    parser.add_argument('--debug', action='store_true', default=True,
                       help='ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”')
    
    args = parser.parse_args()
    
    if args.setup_only:
        setup_only()
        return
    
    print("ğŸš€ FS Master Web Application ì‹œì‘")
    print(f"ğŸ“ ì„œë²„ ì£¼ì†Œ: http://{args.host}:{args.port}")
    print(f"ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: {database_url}")
    
    # PostgreSQL ì—°ê²° ëŒ€ê¸°
    if not wait_for_postgres():
        print("âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
    init_app()
    
    # ì›¹ ì„œë²„ ì‹œì‘ (ê°œë°œ ëª¨ë“œì—ì„œ ìë™ ë¦¬ë¡œë“œ í™œì„±í™”)
    app.run(debug=True, host=args.host, port=args.port, use_reloader=True)

@app.route('/api/debug-scheduler')
def api_debug_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ë””ë²„ê¹… ì •ë³´ API"""
    try:
        # í˜„ì¬ ì‹œê°„
        now_kst = get_kst_now()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
        scheduler_jobs = scheduler.get_jobs()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤ì¼€ì¤„ ì •ë³´
        db_schedules = db.session.query(BatchSchedule).all()
        
        debug_info = {
            'current_time_kst': now_kst.strftime('%Y-%m-%d %H:%M:%S'),
            'scheduler_timezone': str(scheduler.timezone),
            'scheduler_running': scheduler.running,
            'total_scheduler_jobs': len(scheduler_jobs),
            'scheduler_jobs': []
        }
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—… ì •ë³´
        for job in scheduler_jobs:
            job_info = {
                'id': job.id,
                'func': str(job.func),
                'trigger': str(job.trigger),
                'next_run_time': str(job.next_run_time) if job.next_run_time else None,
                'next_run_time_kst': format_scheduler_time(job.next_run_time) if job.next_run_time else None
            }
            debug_info['scheduler_jobs'].append(job_info)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤ì¼€ì¤„ ì •ë³´
        debug_info['db_schedules'] = []
        for schedule in db_schedules:
            schedule_info = {
                'id': schedule.id,
                'job_id': schedule.job_id,
                'cron_expression': schedule.cron_expression,
                'is_active': schedule.is_active,
                'last_run': format_db_kst_time(schedule.last_run) if schedule.last_run else None,
                'next_run': format_db_kst_time(schedule.next_run) if schedule.next_run else None,
                'scheduler_registered': f'schedule_{schedule.id}' in [job.id for job in scheduler_jobs]
            }
            debug_info['db_schedules'].append(schedule_info)
        
        return jsonify(debug_info)
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        })

@app.route('/api/schedule-logs')
def api_schedule_logs():
    """ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ë¡œê·¸ API"""
    try:
        # ìµœê·¼ 24ì‹œê°„ ë™ì•ˆì˜ ìŠ¤ì¼€ì¤„ ê´€ë ¨ ë¡œê·¸
        yesterday = get_kst_now() - timedelta(days=1)
        
        logs = db.session.query(BatchLog).filter(
            BatchLog.schedule_id.isnot(None),
            BatchLog.started_at >= yesterday
        ).order_by(BatchLog.started_at.desc()).all()
        
        log_list = []
        for log in logs:
            log_info = {
                'id': log.id,
                'schedule_id': log.schedule_id,
                'job_id': log.job_id,
                'status': log.status,
                'started_at': format_db_kst_time(log.started_at) if log.started_at else None,
                'completed_at': format_db_kst_time(log.completed_at) if log.completed_at else None,
                'duration_seconds': log.duration_seconds,
                'total_rows': log.total_rows,
                'error_message': log.error_message
            }
            log_list.append(log_info)
        
        return jsonify({
            'success': True,
            'logs': log_list,
            'total_count': len(log_list)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/debug-timezone')
def api_debug_timezone():
    """ì‹œê°„ëŒ€ ë””ë²„ê¹… API"""
    try:
        from datetime import datetime, timezone, timedelta
        
        # í˜„ì¬ ì‹œê°„ë“¤
        utc_now = datetime.now(timezone.utc)
        kst_now = get_kst_now()
        local_now = datetime.now()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì •ë³´
        scheduler_info = {
            'timezone': str(scheduler.timezone),
            'running': scheduler.running,
            'job_count': len(scheduler.get_jobs())
        }
        
        # í™œì„± ìŠ¤ì¼€ì¤„ë“¤ì˜ ì‹œê°„ ì •ë³´
        active_schedules = db.session.query(BatchSchedule).filter_by(is_active=True).all()
        schedule_times = []
        
        for schedule in active_schedules:
            try:
                scheduler_job = scheduler.get_job(f'schedule_{schedule.id}')
                schedule_info = {
                    'id': schedule.id,
                    'cron_expression': schedule.cron_expression,
                    'db_last_run': format_db_kst_time(schedule.last_run) if schedule.last_run else None,
                    'db_next_run': format_db_kst_time(schedule.next_run) if schedule.next_run else None,
                    'scheduler_next_run': format_scheduler_time(scheduler_job.next_run_time) if scheduler_job and scheduler_job.next_run_time else None
                }
                schedule_times.append(schedule_info)
            except Exception as e:
                schedule_times.append({
                    'id': schedule.id,
                    'error': str(e)
                })
        
        return jsonify({
            'success': True,
            'current_times': {
                'utc': utc_now.strftime('%Y-%m-%d %H:%M:%S %Z'),
                'kst': kst_now.strftime('%Y-%m-%d %H:%M:%S %Z'),
                'local': local_now.strftime('%Y-%m-%d %H:%M:%S')
            },
            'scheduler_info': scheduler_info,
            'schedule_times': schedule_times
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

if __name__ == '__main__':
    main() 